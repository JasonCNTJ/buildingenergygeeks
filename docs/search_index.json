[["index.html", "Building energy statistical modelling Foreword", " Building energy statistical modelling Simon Rouchier 2021-06-23 Foreword This website is being rebuilt from scratch. It will hopefully be ready in time for the Building Simulation 2021 conference. Stay tuned! "],["scope.html", "Chapter 1 Background on data analysis 1.1 From data to energy savings 1.2 Inverse problems 1.3 Categories of data-driven modelling approaches", " Chapter 1 Background on data analysis 1.1 From data to energy savings 1.1.1 Formalisation of the system Before proposing a few examples on how data acquisition may support energy conservation measures, let us first formalise the framework, in which the following parts of this book will describe building energy performance. Figure 1.1: Formalisation of the system into observable variables and non-measurable influences The first level of this formalisation are the two conditions imposed on the building: weather and occupancy. What these two terms have in common is the fact that the building’s designer does not get to choose or influence them: every analysis that we will conduct will be conditional on these imposed conditions. The weather imposes the outdoor boundary conditions of the building envelope. It can be described as a set of measurable quantities (outdoor air temperature, humidity, direct and diffuse solar irradiance, wind speed and direction…), some of which are predictable to some extent. The occupancy is more difficult to extensively describe with a finite set of variables. This term is used here to encompass all actions and choices of the occupants regarding their own comfort and how they “operate” the building: leaving and entering the building, setting indoor temperature set points, opening and closing windows, operating appliances that consume energy… These actions cannot be simply measured and summarised into a few descriptive variables, as can be the weather. The second level of the formalisation is the building itself. Building energy simulation usually separates the description of the HVAC systems and the envelope. Both can be characterised in terms of energy performance by a finite set of intrinsic performance indicators: heat transfer coefficient of the envelope, boiler efficiency, window transmissivity, pipe network heat loss… These quantities cannot be directly observed, and define the intrinsic energy performance of the building: their values should not depend on the current state of the two previously mentioned conditions. The third level of the formalisation includes all extensive and intensive variables that can be measured by any kind of meter or sensor, inside or near the building. These variables are the consequences of the two conditions (weather and occupancy) coupled with the building’s intrinsic performance. They include readings from energy meters, variables which describe the indoor air (temperature, relative humidity, CO\\(_2\\) concentration…) and various signals that describe the state of operation of HVAC systems or envelope components. The indoor air temperature, for instance, is influenced by the weather (outdoor air temperature, solar irradiance), the occupants (who chose the set point) and the intrinsic building energy performance (heat transfer coefficient and inertia of the envelope, efficiency of the temperature control loop). These variables, along with weather variables, constitute the data from which we will perform inferences and predictions. 1.1.2 Some uses of data We now use this formalisation to demonstrate a few examples of how recorded data may be used to motivate energy conservation measures or verify their efficiency. Figure 1.2: How data can be used for various inferences and predictions Forecasting energy use The ability to predict the energy use of buildings is a useful tool for energy management, from the scale of a single dwelling to the scale of energy distribution in a smart grid. On a side note, we can mention a difference between the terms of prediction and forecasting. In modelling studies, prediction is a general term that means computing the outcome of any simulation model, while forecasting specifically denotes estimating the future values of a time series, on a time horizon where observations are not available. Two factors may facilitate the ability to forecast a time series. The first of these advantageous characteristics is a repetitive trend in an energy consumption profile. Large office buildings, collective housing and retail facilities tend to display a predictable daily profile for energy uses that have a low dependency on environmental factors, such as lighting and electrical appliances. Some energy uses mostly depend on user behaviour, whose stochastic behaviour tend to be smoothed out at larger scales of observation. Figure 1.3: Repetitive consumption profiles are easy to forecast by extrapolating daily and seasonal fluctuations Another factor that facilitates the prediction of energy use is a high dependency on an environmental factor that is itself easy to get forecasts of. In a building with a controlled set-point indoor temperature, a strong correlation may be observed between the outdoor temperature and the heating power in winter. The outdoor temperature is then considered a significant explanatory variable: its forecasts will allow forecasting the heating power with a satisfactory confidence. Figure 1.4: A correlation between outdoor temperature and heating power can be used to predict future demand These two examples illustrate two categories of leverages in time series analysis and forecasting. When forecasting a particular series, we can either make use of its own characteristics (periodicity, seasonality, autocorrelation…), or identify dependencies with other measurable data. Measurement and verification (M&amp;V) The ability to predict energy demand can also be valued in the context of Measurement and Verification (M&amp;V). M&amp;V is the process of assessing savings caused by an Energy Conservation Measure (ECM). Savings are determined by comparing measured consumption or demand before and after implementation of a program, making suitable adjustments for changes in conditions. The International Performance Measurement and Verification Protocol (IPMVP) formalizes this process, and presents several options to conduct it. Figure 1.5: The IPMVP provides guidelines on how to perform M&amp;V An example of adjustment is, when estimating the energy savings delivered by an ECM, to substract its new energy consumption from the consumption that would have occurred if the building had stayed in the same situation in the weather conditions of the reporting period (adjusted baseline consumption). This requires a prediction model that can extrapolate the initial behaviour of the building by accounting for variable weather conditions. Other possible adjustments include changes in occupancy schedules. This is necessary to assess whether measured energy savings are caused by the ECM itself, or by changes in these influences. The IPMVP presents several options, depending on whether the operation concerns an entire facility or a portion, and defines the notion of measurement boundary as the set of measurements that are relevant to determine savings. In order to verify the savings from a single equipment, and a measurement boundary can be drawn around it, the approach used is retrofit-isolation: IPMVP options A and B. If the purpose of reporting is to verify total facility energy performance, the approach is the whole-facility option C. All options must account for the presence of interactive effects: energy impacts created by the ECM that cannot be measured within the measurement boundary. Any option that requires adjustment on measured independent variables implies the use of a prediction model, even a simple one. In the IPMVP option D, savings are determined through simulation of the energy consumption rather than direct measurements. The simulation model is calibrated so that it predicts the energy and load that matches the actual metered data. Under the correct assumptions, and with the right methodology (which we propose in this book!), calibrated simulation is potentially a very powerful M&amp;V methodology, as it may disaggregate energy uses and estimate interactive effects outside of the measurement boundary. M&amp;V is a crucial tool in the establishment of Energy Performance Contracts (EPC), which can be established on the basis of designed performance (building energy simulation), with an eventual uncertainty analysis and/or sensitivity analysis, or on the basis of measured energy consumption. Intrinsic performance assessment The third hereby presented application of data analysis is the estimation of quantitative indicators of the intrinsic energy performance of a building. This is different from the M&amp;V process as it does not necessarily imply the comparison between before/after situations. One typical example is the Heat Loss Coefficient (HLC), or the Heat Transfer Coefficient, which characterize heat transmission through the envelope, eventually including air infiltration. The co-heating test is one of the well-known methods for HLC assessment: it measures the heating power required to maintain a steady indoor temperature, and obtains HLC by averaging these measurements over a sufficiently long period. Figure 1.6: The co-heating test records the heating power, indoor and outdoor temperature, in order to estimate the heat loss coefficient of the envelope What is referred here as intrinsic performance assessment may also be called characterization, or parameter estimation, since it primarily works by estimating the values of static parameters of a simulation model. Such parameters may include the HLC, but also the efficiency of a system, an air infiltration rate… Hence, it can be part of an energy audit to help characterize the state of a building and its eventual flaws before the design of an ECM. Fault detection and diagnostics (to be completed) Figure 1.7: Fault detection is a double challenge: (1) automatically detecting a difference between measurements and normal behaviour; (2) identifying its possible cause. 1.2 Inverse problems 1.2.1 Model calibration as the key to data analysis All the applications described above can be summarized by the same description: data are recorded and interpreted to draw conclusions about quantities that are either not directly observable (such as estimating a heat loss coefficient), or not yet observed (such as forecasting energy use). In all cases, the missing link between the data and the conclusion is a numerical model. Figure 1.8: The overall process of collecting and analyzing data As was described by the formalisation proposed above, the energy consumption of HVAC systems and other appliances, along with other measurable indoor variables, are the consequences of two sets of conditions (weather and occupancy) and of the intrinsic energy performance of the envelope and systems. Disaggregating each of these influences, and accessing intrinsic energy performance indicators, is a major challenge. For instance, measurement and verification protocols attempt to demonstrate whether energy savings may be attributed to an energy conservation measure, or is partially caused by a change in weather or occupancy behaviour. After identifying the phenomena that we wish to predict, or the performance indicators that we wish to estimate, monitoring equipment is implemented for data acquisition. Measurements are an insight of the real behaviour of a building, and are the basis for training the models that will reproduce it. The required types of monitoring depend on the characterisation target and on the specific energy uses of the building under study. In all cases, measurements can only provide a very fractional view of all phenomena that drive the energy performance. Heating and cooling energy consumption, for instance, is the outcome of several concurring heat transfer phenomena: transmission through the envelope and between thermal zones; convection through ventilation and air infiltration; temperature stratification inside each room; long-wave radiative heat exchange between walls… Heating and cooling are also not the only energy consumptions that an operator may wish to be able to predict. Unless separate energy meters are implemented on each system and appliance, measurements of energy consumption are often aggregated values from which separate uses are difficult to isolate. Other important characteristics of the monitoring equipment are: the type and accuracy of sensors used for a given measurement, the acquisition time step, the spatial granularity of observation. Raw data only describes a fraction of the overall system, and does not allow disaggregating intrinsic energy performance indicators from influences of the weather and of the occupants. The solution to this problem is to define a numerical model as the missing link between the complexity of the real building and the conciseness of the data. The model is a numerical description of the building, where the non-observable performance indicators are given a specified value. The conditions imposed by the weather and the occupants are quantified in the equations of the model, which in turn provides values for energy consumption or indoor variables as output. Model specification is all but trivial, especially because of the variety of model types offered by building energy simulation. Selecting an appropriate model structure is essential to the learning procedure. The complexity of the model is a compromise between realism and parcimony: it should at least describe all the most significant processes occuring in the system, and should not allow any redundancy in the input-output relationship. Among several models, equally capable of reproducing a dataset, the best choice is usually the most simple one (Hastie, Tibshirani, and Friedman (2009)). The model is first defined after our knowledge of the state of the building. The next step is its calibration using the measured data. Calibrating a model means finding the settings or set of parameters with which its output best matches a series of observations, called a training dataset. This data usually originates from measurements (in either experimental test cells or real buildings), but may also have been produced by a complex reference model that we wish to approximate by a simplified one. There are two main outcomes of model training, which were already shown by two categories of data analysis applications: The first outcome are inferences about the processes that generated the data. If the model structure was appropriately chosen for this purpose, its parameters are related to the intrinsic energy performance indicators of the building. The second outcome is the ability acquired by the model to reproduce measurements, and therefore forecast the future values of some of the observed variables. Model calibration is therefore the key to all applications of data analysis that we mentioned earlier. It is however a more complicated problem than it seems and requires careful choices at every step of the entire procedure. 1.2.2 The difficulty of inverse problems Inverse techniques are a suite of methods which promise to provide better experiments and improved understanding of physical processes. Inverse problem theory can be summed up as the science of training models using measurements. The target of such a training is either to learn physical properties of a system by indirect measurements, or setting up a predictive model that can reproduce past observations. In the last couple of decades, building physics researchers have benefited from elements of statistical learning and time series analysis to improve their ability to construct knowledge from data. What is referred to here as inverse problems are actually a very broad field that encompasses any study where data is gathered and mined for information. Inverse heat transfer theory (Beck, Blackwell, and Clair Jr (1985)) was developed as a way to quantify heat exchange and thermal properties, and has translated well into building physics. Many engineers and researchers however lack the tools for a critical analysis of their results. This caution is particularly important as the dimensionality of the problem (i.e. the number of unknown parameters) increases. When data are available and a model is written to get a better understanding of it, it is very tempting to simply run an optimisation algorithm and assume that the calibrated model has become a sensible representation of reality. If the parameter estimation problem has a relatively low complexity (i.e. few parameters and sufficient measurements), it can be solved without difficulty. In these cases, authors often do not carry a thorough analysis of results, their reliability and ranges of uncertainty. However, it is highly interesting to attempt extracting the most possible information from given data, or to lower the experimental cost required by a given estimation target. System identification then becomes a more demanding task, which cannot be done without proof of reliability of its results. One should not overlook the mathematical challenges of inverse problems which, when added to measurement uncertainty and modelling approximations, can easily result in erroneous inferences. Figure 1.9: Inverse problems in a nutshell Following the formalisation of building energy monitoring shown above, we propose a formalisation of a typical inverse problem for building physics (Rouchier (2018)), without considering any statistical aspects for now. The general principle of solving a system identification problem is to describe an observed phenomenon by a model allowing its simulation. Measurements \\(\\mathbf{z}=(\\mathbf{u},\\mathbf{y})\\) are carried in an experimental setup: a building is probed for the quantities from which we wish to estimate its energy performance (indoor temperature, meter readings, climate, etc.) A model is defined as a mapping between some of the measurements set as input \\(\\mathbf{u}\\) (boundary conditions, weather data) and some as output \\(\\mathbf{y}\\). A numerical model is a mathematical formulation of the outputs \\(\\hat y(u, \\theta)\\), parameterised by a finite set of variables \\(\\theta\\). The most intuitive way to calibrate a model is to minimize and indicator such as the sum of squared residuals with an optimisation algorithm, in order to find the value of \\(\\theta\\) that makes the model most closely match the data. Ideally, the model is unbiased: it accurately describes the behaviour of the system, so that there exists a true value \\(\\theta^*\\) of the parameter vector for which the output \\(\\hat y\\) reproduces the undisturbed value of observed variables. \\[\\begin{equation} \\mathbf{y}_k = \\mathbf{\\hat y}_k(u, \\theta^*) + \\varepsilon_k \\tag{1.1} \\end{equation}\\] where \\(\\varepsilon\\) denotes measurement error, i.e. the difference between the real process \\(y^*\\) and its observed value \\(y\\). The most convenient assumption is that of additive noise, i.e. \\(\\varepsilon_k\\) is a sequence of independent and identically distributed random variables. In practice, \\(\\theta^*\\) will never be reached exactly, but rather approached by an estimator \\(\\hat \\theta\\), because the entire process of estimating it from measurements is disturbed by an array of approximations (Maillet (2010)) Experimental errors. The numerical data \\((u,y)\\) available for model calibration differs from the hypothetical outcome of the ideal, undisturbed physical system \\((u^*,y^*)\\). Sensors may be intrusive, produce noisy measurements, may be poorly calibrated, have a finite precision and resolution… Numerical errors. The hypothesis of an unbiased model (Eq. ) states that there exists a parameter value \\(\\theta^*\\) for which the model output is separated from the observations \\(y\\) only by a zero mean, Gaussian distributed measurement noise. It means that the model perfectly reproduces the physical reality, and the only perceptible error is due to the imperfection of sensors. This is exceedingly optimistic, especially in building energy simulation. Measurement and modelling approximations are problematic because inverse problems are typically ill-posed (Beck, Blackwell, and Clair Jr (1985)): their solution is highly sensitive to noise in the measured data and approximation errors. A global optimum of the inverse problem may then be found with unrealistic physical values for \\(\\theta\\) as a consequence of seemingly moderate errors made when setting up the problem. Figure 1.10: Errors and uncertainties on parameter estimation, caused by measurement and modelling errors We divided source of errors into experimental and numerical errors. The Guide to the expression of Uncertainty in Measurement (GUM)(JCGM (2008)) then separates errors into a random component and a systematic component: systematic errors are errors which retain a non-zero mean if the measurement was repeated an infinite number of times under repeatability conditions. Systematic and random errors, whether they concern the measurement or the modelling procedures, will affect the estimation of a parameter \\(\\theta\\) in terms of accuracy and precision. The figure above illustrates accuracy and precision in the case of estimating a parameter value \\(\\theta\\), but the exact same terminology can be used if the purpose of the trained model is to predict the future values of a variable \\(y\\). The GUM defines uncertainty (of measurement) as the dispersion of the values that could reasonably be attributed to a measured quantity. Similarly, parameter estimates or model predictions come with an uncertainty, which quantifies their possible range of values caused by the random errors in the measurement and modelling processes. Precision is an indicator of low uncertainty, and can be conveyed by confidence intervals. On the other hand, accuracy is a measure of bias. It is the difference between the “true” value of the target variable and the mean of our estimation. Biased estimates and predictions are the outcome of errors that have not been explicitely taken into account in the inverse problem. We tend to prefer low bias and high uncertainty, than high bias and low uncertainty: indeed, a high uncertainty suggests that the data were not sufficient to provide confident inferences, which incites caution when communicating the results. On the contrary, the bias cannot be simply estimated and is not visible. The worst case scenario is obtaining a bias higher than the uncertainty, which means that the true reference value is not even contained in our confidence interval. By including possible systematic errors in the model formulation, we wish to “turn bias into uncertainty”. In order to ensure, as much as possible, that the parameters and predictions returned by the model calibration procedure are unbiased and physically interpretable, a complete workflow will be described in this book. This workflow sums up the important steps that should be followed before and after applying the training algorithm itself, and the various tests to be performed to prevent biased conclusions. Statistical modelling will give us the tools to perform such a careful analysis of data. 1.3 Categories of data-driven modelling approaches 1.3.1 Either physical interpretability or prediction accuracy The main subject of this book is to propose a workflow for the analysis of building energy data, that attempts to make the most out of the available data while avoiding the inherent pitfalls of inverse problems. This workflow is described and applied in the parts of the book that follow. Before presenting this workflow, it is however perhaps necessary to clarify some aspects of vocabulary. The previous sections have used expressions that seemed interchangeable, or at least overlapping in their definitions: model calibration; data-driven modelling; statistical learning and inference; inverse problems… These terms can describe the same process, or a part of it: collecting data and interpreting them with a numerical or a statistical model, in order to draw conclusions that will support energy conservation measures. There is so much literature on data-driven approaches for forecasting building energy consumption and demand, that several reviews are made every year, and a review of these reviews could be done. One noticeable trend is to classify models into white-box, grey-box and black-box (Deb and Schlueter (2021)), according to their physical interpretability. A classification of data analysis methods and terms is proposed here: models categories from white-box to black-box are shown on a scale of two criteria: physical interpretability and forecasting accuracy. They are then roughly separated into three types of approaches: model calibration (mostly for white-box models), machine learning (black-box) and statistical learning with (grey-box) probabilistic models). Figure 1.11: Data analysis methods can be split into three categories. The first criterion by which methods can be classified is their requirements. These applications shown above essentially have at least one of the following two requirements: Applications that require the ability to accurately forecast the energy use, or any other variable: energy management, optimised predictive control. These applications do not need the trained predictive model to have physically interpretable parameters, or even parameters at all. Applications that involve learning the value of one or more interpretable physical values that describe physical properties of a building. These applications, such as the co-heating test, may be denote performance assessment or characterisation. Some applications require both prediction accuracy and physical interpretability to some extent: measurement and verification, commissioning, fault detection… The requirement of data analysis will determine the type of model that will be trained to replicate the data. The type of model is then closely related to the way that inferences will be drawn from it. We can loosely classify data analysis workflows into the three following categories: 1.3.2 Calibrated simulation (white-box) Model calibration usually denotes fitting numerical models which are based on a more or less detailed physical description of the building, complete with a description of HVAC systems and controls, usually without a statistical representation of variables. The advantage of these models is their interpretability: each parameter has a direct meaning, which can be related to thermophysical properties of the envelope or systems. The IPMVP Option D evaluates energy savings by directly adding or removing energy conservation measures in a calibrated numerical model, and therefore requires such a detailed description of the building. Detailed building energy models can be trained to replicate data by manual adjustement of parameters, or by more automated methods (Reddy (2006)). The ASHRAE Guideline 14 specifies what is an acceptable level of accuracy or uncertainty for a calibrated simulation with very permissive criteria: “typically, models are declared to be calibrated if they produce Mean Bias Errors (MBE) within 10% and CV(RMSE) within 30% when using hourly data, or 5% MBE and 15% CV(RMSE) with monthly data.” CV(RMSE) is defined by: \\[\\begin{equation} \\mathrm{CV(RMSE)} = 100 \\frac{\\sqrt{\\sum\\left(y_i-\\hat{y}_i\\right)^2}}{\\bar{y}\\sqrt{n-p}} \\end{equation}\\] where \\(y_i\\) are measurement points, \\(\\hat{y}_i\\) are model predictions, \\(n\\) the number of data points and \\(p\\) the degrees of freedom of the model. Calibrating detailed models however comes with conditions and limitations. A sufficiently detailed model, with enough degrees of freedom, will have no difficulty satisfying the above criteria, but may do so without necessarily assigning their true physical value to each parameter. This inverse problem may have identifiability issues, i.e. the existence of infinitely many combinations of different parameters which result in the same model output. A preliminary sensitivity analysis may be conducted in order to only select the most significant parameters as free for calibration, while fixing the rest. Still, deterministic models greatly underestimate the bias and uncertainty of their own predictions, and calibrated simulation can easily satisfy ASHRAE’s validation criterion without representing the true state of a building. 1.3.3 Machine learning (black-box) On the other side of the spectrum, machine learning (ML) is a purely data-driven approach. ML models are not based on physical considerations, but are designed to replicate observed patterns with maximum flexibility and adaptability. The most popular choices of ML methods are Artificial Neural Networks, Support Vector Machines, Boosting and Random Forests. One of the main references on the field is The Elements of Statistical Learning by (Hastie, Tibshirani, and Friedman (2009)). As mentioned earlier, there is enough literature on data-driven building energy modelling to motivate a review of reviews (Amasyali and El-Gohary (2018)), even when only focusing on the machine learning (black-box) side. Because of their lack of physical interpretability, it is difficult for trained ML models to provide insight into thermophysical properties of building components. For this reason, their applications are complementary to the energy model calibration approach mentioned in the previous section. However, they are designed for prediction: they are well suited for forecasting energy demand. ML also comes with standard practices of validation and model order selection, in order to find a good bias-variance tradeoff and ensure accurate predictions. This book will not venture very far into machine learning territory. We will however use Gaussian Process models at some point, by integrating them into other statistical models rather than by themselves. 1.3.4 Statistical modelling and inference (grey-box) Calibrated simulation and machine learning both have advantages and limitations, as they are either appropriate for parameter interpretability or prediction accuracy. This book will focus on the third option: probabilistic modelling and statistical inference. Statistical inference can either follow a frequentist or a Bayesian paradigm: both will be introduced and demonstrated in our applications. Statistical models represent the data-generating process (the building) as a set of statistical assumptions and stochastic processes, rather than deterministic relationships between variables. The formulation of these stochastic processes can be based on physical considerations, like a typical building energy model, except that they explicitely include possible errors and uncertainty. As a result, parameter estimates and predictions are inferred with a certain uncertainty as well, which translates the confidence that our model is able to produce about them. Model checking criteria then allow us to anticipate possible bias: results produced by a thoroughly validated statistical inference procedure are more reliable than deterministic calibrated simulation. Probabilistic modelling starts with the definition of an sampling distribution \\(p\\left(y|\\theta\\right)\\), which is the distribution of the observed data \\(y\\) conditional on the model parameters \\(\\theta\\). When viewed as a function of \\(\\theta\\) for fixed \\(y\\), this distribution is called the likelihood function. Finding the value of \\(\\theta\\) that maximizes the likelihood function is called Maximum Likelihood Estimation, one of the main cases of frequentist inference. Bayesian inference adds a prior probability distribution \\(p(\\theta)\\) to the problem. The prior distribution describes any knowledge we may already have regarding the model parameters, before accounting for the measured data. According to (Gelman et al. (2013)): “Bayesian inference is the process of fitting a probability model to a set of data and summarizing the result by a probability distribution on the parameters of the model and on unobserved quantities such as predictions for new observations”. Therefore, another specificity of Bayesian inference compared to frequentist inference is the fact that all variables of the problem are described as probability distributions, rather than point estimates. This book is focused on statistical modelling and inference applied to building energy performance assessment. The next chapter will now describe how building physics can be formulated with statistical models, and present a few possible structures for these models. Then, we will propose a full workflow for statistical inference, either frequentist or Bayesian, which aims at making sure that models are well defined and trained for a given application. References "],["models.html", "Chapter 2 Building energy statistical modelling 2.1 Building physics in a nutshell 2.2 Measurement and modelling boundaries 2.3 Categories of statistical models", " Chapter 2 Building energy statistical modelling The positioning of this book is statistical modelling and inference, applied to building energy performance assessment. In the previous chapter, this approach was presented as a compromise between physical interpretability of parameter estimates, and flexibility of predictive models. The first step into this approach, before taking data into consideration, is the probabilistic modelling of the energy balance of buildings. Rather than using Building Energy Simulation (BES) software, our approach is generally to start from simple models and gradually increase complexity if required. This means writing each equation individually, and formulating uncertainty into the probabilistic framework. 2.1 Building physics in a nutshell BES decomposes a building into separate thermal zones, each of which is assumed to have a uniform air temperature. A single-family house may be split into one or two zones, while larger housings and office buildings have more, usually denoted by their orientation and usage. The temporal evolution of temperature, humidity and other variables of comfort or indoor air quality, are calculated in each zone as consequences of: influence of the weather; exchange between zones; HVAC settings; occupancy. BES software can reach quite a high level of detail when modelling the phenomena that influence these indoor variables: long-wave radiative heat exchange between all walls; position of the sunspot; influence of furniture on indoor humidity; CFD modelling of air flow… Since we will give our building energy models a statistical formulation, these models should stay as simple as possible, while staying close to the main phenomena that govern the heat and energy balance of an observed building. A schematic outlook of heat gains and losses in a heated space is shown here, which we will summarise by three main equations (Fig. 2.1 below assumes the example of a heated building in which gas is the main fuel used by the heating appliances. A similar reasoning can be done in other conditions, such as an air conditioned building in summer). Figure 2.1: Decomposition of heat gains and losses in a heated zone The first equation of our simplified building energy modelling framework is the temporal evolution of the indoor temperature in a thermal zone. It is shown on the right side of Fig. 2.1 by the imbalance between all heat gains (\\(\\Phi_\\mathit{in}\\), red lines) and all heat losses (\\(\\Phi_\\mathit{out}\\), blue lines). \\[\\begin{equation} C \\frac{\\partial T}{\\partial t} = \\Phi_\\mathit{in} - \\Phi_\\mathit{out} \\tag{2.1} \\end{equation}\\] Where \\(C\\) is an effective heat capacity of the thermal zone. The second equation is the breakdown of \\(\\Phi_\\mathit{in}\\), which includes all heat gains of the room other than transmission and ventilation exchanges. This part is shown by red lines on Fig. and will depend on the specificities of each building. A common list of usual heat inputs can be the following. \\[\\begin{equation} \\Phi_\\mathit{in} = \\Phi_h + \\Phi_\\mathit{sol} + \\Phi_\\mathit{int} \\tag{2.2} \\end{equation}\\] \\(\\Phi_h\\): the energy consumption dedicated to space heating. This value is usually not directly measured, but is only a part of a meter reading (e.g. gas) which includes production and distribution losses, and often also cover the production of domestic hot water (DHW). \\(\\Phi_\\mathit{sol}\\): the solar heat gains. They are the outcome of direct and diffuse solar radiation, which are measured outside. The fraction of the total outdoor solar irradiance which is converted into indoor heat input depends on the orientation of the room relative to the position of the sun, the shadings, the type of glazing, etc. \\(\\Phi_\\mathit{int}\\): the sum of other internal heat gains, due to the presence of inhabitants, water heat gains, electrical appliances, lighting, etc. This variable is difficult to measure but tends to show a daily or weekly pattern, which makes it predictable to some extent. The third main equation is the breakdown of \\(\\Phi_\\mathit{out}\\), which includes all heat exchange between the zone under consideration and its surroundings: \\[\\begin{equation} \\Phi_\\mathit{out} = H_\\mathit{tr}^e \\left(T-T_e\\right) + H_\\mathit{tr}^s \\left(T-T_s\\right) + \\sum_j H_\\mathit{tr}^\\mathit{adj,j} \\left(T-T_\\mathit{adj,j}\\right) + \\Phi_\\mathit{inf} + \\Phi_v \\tag{2.3} \\end{equation}\\] The first term \\(H_\\mathit{tr}^e \\left(T_i-T_e\\right)\\) denotes heat loss by direct transmission from the heated room at temperature \\(T\\) to the outside at temperature \\(T_e\\). The \\(H_\\mathit{tr}^e\\) coefficient includes the heat transmissivity of opaque walls, glazing and thermal bridges. The second term \\(H_\\mathit{tr}^g \\left(T_i-T_g\\right)\\) denotes heat loss towards the ground at temperature \\(T_g\\). The third term encompasses heat exchange with all adjacent rooms. This mainly concerns unheated spaces, which may have a significant temperature difference with the thermal zone under consideration. \\(\\Phi_\\mathit{inf}\\) et \\(\\Phi_\\mathit{ven}\\) respectively denote heat loss from air infiltration or mechanical ventilation. These terms are written here in the direction of heat leaving the room, hence the name of the variable \\(\\Phi_\\mathit{out}\\). This is of course just a notation: if the outdoor air temperature or an adjacent room temperature are higher than the zone’s temperature \\(T\\), some of these terms may very well switch signs, implying that heat is entering the zone. These three first equations (2.1) to (2.3) will be the basis for all our modelling of heat transfer. Fig. 2.1 also illustrates some challenges of statistical inference for building energy performance assessment. An important question is the difficulty to directly observe the terms of Eq. (2.2) that influence the indoor heat balance. On the one hand, some hypotheses are required to formulate the solar heat gains \\(\\Phi_\\mathit{sol}\\) from outdoor measurements of solar irradiance. On the other hand, the internal heat gains \\(\\Phi_\\mathit{int}\\) of occupied buildings are the sum of influences that are hard to measure. Even the heating power \\(\\Phi_h\\) is usually not directly available. Still in the example of a building equipped with a hydronic heating system fueled by gas, a typical situation is having a common meter for all of gas consumption. \\[\\begin{equation} e_\\mathit{gas}(t) = e_\\mathit{dhw}(t) + e_\\mathit{sh}(t) + e_\\mathit{loss}(t) \\tag{2.4} \\end{equation}\\] The consumption intended for domestic hot water production \\(e_\\mathit{dhw}\\) and space heating \\(e_\\mathit{sh}\\) either need to be metered separately, or to be disaggregated from a single meter \\(e_\\mathit{gas}\\). Some options for this disaggregation will be mentioned in Chap. . Then, the energy consumption for space heating \\(e_\\mathit{sh}\\) translates to the heating power \\(\\Phi_h\\) (see Eq. (2.2)) through assumptions regarding the heating system. If the target of a study is the performance assessment of the buiding envelope from measurements of heating power and temperatures, then different strategies will be required if \\(\\Phi_h\\) is somehow directly measured, than if only a general meter for \\(e_\\mathit{gas}\\) is available. The proper formulation of Eq. (2.2) therefore requires assumptions to translate outdoor solar irradiance measurements into \\(\\Phi_\\mathit{sol}\\), and assumptions to translate energy meter readings into \\(\\Phi_h\\). The second challenge we mention here is the prediction of electricity consumption, and eventually its impact on the indoor heat balance. Supposing that hourly or daily measurements of electricity consumption are available, one can be interested in either: identifying repetitive patterns which makes this consumption predictable for purposes of energy distribution management; estimating the fraction of this energy use that contributes to indoor heat gains \\(\\Phi_\\mathit{int}\\). The former question is closely related to the detection and data-driven modelling of occupancy, which is a ML problem that may be based on a variety of sensors and methods. The second question is even more challenging, as it requires not only an estimation of the use of each appliance, but also of their heat loss percentage. The third challenge displayed on Fig. 2.1 is the decomposition of the heat loss of the envelope. The first three terms on the right side of Eq.(2.3) may be aggregated in order to define two global indicators of the heat performance of the envelope: the total Heat Transfer Coefficient (HTC) and the transmission coefficient \\(H_\\mathit{tr}\\). \\[\\begin{align} \\Phi_\\mathit{out} &amp; = \\underbrace{H_\\mathit{tr} \\left(T-T_e\\right) + \\Phi_\\mathit{inf}}_{\\mathrm{HTC} \\left(T-T_e\\right)} + \\Phi_v \\label{eq:HTC}\\\\ \\mathrm{HTC} &amp; = H_\\mathit{tr} + \\Phi_\\mathit{inf}/\\left(T-T_e\\right) \\tag{2.5} \\end{align}\\] The \\(H_\\mathit{tr}\\) coefficient describes all heat transmission through the envelope, and the HTC also includes the effect of air infiltration. Controlled mechanical ventilation is not included in these coefficients, but may as well be considered as part of the heat gains in Eq. (2.2). One of the essential questions of this book will be the characterisation of HTC and \\(H_\\mathit{tr}\\), using short-term or long-term measurements that may be recorded without disturbing the normal operation of the building. 2.2 Measurement and modelling boundaries The first step into setting up a probability model is the choice of its boundaries, i.e. which of the measured data is the dependent variable, and which are the explanatory variables. The dependent variable \\(y\\), or model output, is a variable that we wish for a fitted model to be able to predict (this book does not cover situations with several dependent variables in a single model). The explanatory variables, or independent variables, are the model inputs by which we try to explain the evolutions of the dependent variable. Explanatory variables are denoted \\(x\\) in most regression models, or \\(u\\) in more complex hierarchical models where \\(x\\) may denote a latent variable instead. Some models have latent variables, which are unobserved and affect the dependent variable. The IPMVP defines measurement boundaries as “notional boundaries drawn around equipment, systems or facilities to segregate those which are relevant to saving determination from those which are not. All Energy Consumption and Demand of equipment or systems within the boundary must be measured or estimated. […] Any energy effects occurring beyond the selected measurement boundary are called interactive effects. The magnitude of any interactive effects needs to be estimated or evaluated to determine savings associated with the ECMs.” The same definition of boundaries work for simulations: a model must be defined so that its inputs and outputs are the measured independent and dependent variables, and all energy effects occurring within these boundaries are either fixed, or part of the list of parameters \\(\\theta\\) that will be estimated by calibration. Figure 2.2: Building energy models simulate the interactions between envelope, ambiance, HVAC systems and their controls, described by a finite set of parameters. The explanatory variables are related to the two conditions mentioned earlier: weather and occupants. The dependent variables are separate energy consumptions. Typical modelling boundaries of BES resemble Fig. 2.2. The time-varying inputs provided by the user are weather files and occupancy profiles. Most of the time, the latter come from standard scenarios rather than measurement. Occupancy is understood by BES as a finite set of actions and influences: presence, temperature set-points, use of appliances. The model returns predictions of energy use, usually with a higher level of disaggregation (consumption of each system) than is easily available by measurement. Other models can have the indoor temperature as output (Fig. 2.3): for instance, heat transfer simulation models used for assessing the performance of the envelope, or for tuning model predictive control strategies. Figure 2.3: Assessing the performance of the envelope may involve models with the indoor temperature as dependent variable. This example illustrates a problem: the boiler that produces space heating and domestic hot water can be beyond the measurement boundary, and only the global consumption is observed. If the target of a study is to characterise some parameter \\(\\theta\\), or evaluate the evolution of a latent variable, rather than train a predictive model, then the same dataset \\(\\mathcal{D}\\) can be mapped into input and output variables in different ways. Regardless of this choice, the principles of the above definition of measurement boundaries should be applied to modelling: any effects that are believed to influence the dependent variable should be either measured (explanatory variables), given assumed values (interactive effects), or estimated by fitting. 2.3 Categories of statistical models Once the modelling boundaries are set, the next step is choosing the model structure itself, i.e. the equations that relate dependent variables to independent variables and eventual latent variables. The next few sections will describe the different categories of models that will be implemented later in the applications, also summarized by Fig. 2.4. Before getting to these descriptions, a summary of some notations and vocabulary they have in common might be helpful. Figure 2.4: This book separates statistical models in categories according to two questions: whether the time resolution of data is high enough so that temporal dependencies should be accounted for; whether there is a direct relationship between dependent and independent variables or an indirect relationship through latent variables. Gaussian Process models will be presented separately because they can apply to most situations. A regression model directly relates the dependent variable with one or several explanatory variables \\(x\\) and its parameters \\(\\theta\\). Regression concerns dependent variables with continuous values, as opposed to classification which concerns categorical or discrete dependent variables. \\[\\begin{equation} y \\sim f\\left(x, \\theta\\right) \\tag{2.6} \\end{equation}\\] In this form, regression models assumes the independence of all elements of \\(y\\) with each other: each measurement is unaffected by its previous value. These models are therefore to be used with low frequency or aggregated data for long-term predictions or summaries. Some problems can be modelled hierarchically, with observable outcomes modeled conditionally on certain parameters \\(\\theta\\), which themselves are given a probabilistic specification in terms of further parameters \\(\\phi\\), known as hyperparameters. \\[\\begin{equation} \\theta \\sim f\\left(\\phi\\right) \\tag{2.7} \\end{equation}\\] The hyperparameter \\(\\phi\\) can then be given a hyperprior distribution \\(p\\left(\\phi\\right)\\). As written by (Gelman et al. (2013)), “simple nonhierarchical models are usually inappropriate for hierarchical data: with few parameters, they generally cannot fit large datasets accurately, whereas with many parameters, they tend to overfit such data […]. In contrast, hierarchical models can have enough parameters to fit the data well, while using a population distribution to structure some dependence into the parameters, thereby avoiding problems of overfitting.” Hierarchical thinking gives flexibility to simple model structures, and will be useful to explain data from a group of buildings, or from a single building monitored over several operating conditions. Data often comes at high enough frequency so that consecutive measurements of the outcome variable cannot be considered independent from each other. Time-series models offer many ways to express this dependency Shumway and Stoffer (2000) \\[\\begin{equation} y_t \\sim f\\left(y_{t-1}, y_{t-2},..., x, \\theta\\right) \\tag{2.8} \\end{equation}\\] This type of model is called autoregressive because of the similarity of this formulation with the regression model of Eq. (2.6): the dependent variable at time \\(t\\) is a regression function of its previous values. The simplest autoregressive models lack explanatory variables \\(x\\), and only formulate the dependent variable as a regression function of its previous values. They can be used to identify trends and repetitive cycles in a single variable and predict its future values. The last criterion we are considering for classifying models by categories, is the presence of latent variables. A latent variable model is a hierarchical model which relates the observed dependent variable to a set of unobservable latent variables. For each outcome \\(y_n\\) there is a latent variable \\(z_n\\) in \\(\\left\\{1,...,K\\right\\}\\) with a categorical distribution parameterized by some parameter \\(\\lambda\\) (Team (n.d.)) \\[\\begin{align} y_n \\sim f(z_n) \\\\ z_n \\sim \\mathrm{categorical}(\\lambda) \\end{align}\\] Finite mixture models can be parameterized as latent variable models, although the description we will make of them does not explicitely display latent variables. The models which will play the largest role in the next few chapters of this book are time series models with latent variables, or state-space models (SSM). \\[\\begin{align} y_t \\sim f\\left(z_t, \\theta\\right) \\\\ z_t \\sim f\\left(z_{t-1}, \\theta\\right) \\end{align}\\] An SSM is a type of Dynamic Bayesian Network (DBN)(Murphy (2002)) where an underlying hidden state \\(z_t\\), generates the observations \\(y_t\\). The state evolves in time as a function of observable inputs. State-space models expand the classical time-series modelling approaches by allowing more complex assumptions on the evolution of the system and its uncertainty. A Hidden Markov Model (HMM) is a type of DBN whose hidden state takes discrete values. We will use the term of state-space model for models whose hidden states are continuous, although some authors call them Kalman filter models (KFM). DBNs with both categorical and continuous hidden states are called switching dynamic systems or switching Kalman filter models, and are the highest complexity we will consider in this book. References "],["workflow.html", "Chapter 3 A Bayesian data analysis workflow 3.1 Bayesian inference summarised 3.2 Workflow for one model 3.3 Model assessment and selection", " Chapter 3 A Bayesian data analysis workflow 3.1 Bayesian inference summarised 3.1.1 Motivation for a Bayesian approach Bayesian statistics are mentioned in the Annex B of the ASHRAE Guideline 14, after it has been observed that standard approaches make it difficult to estimate the savings uncertainty when complex models are required: “Savings uncertainty can only be determined exactly when energy use is a linear function of some independent variable(s). For more complicated models of energy use, such as changepoint models, and for data with serially autocorrelated errors, approximate formulas must be used. These approximations provide reasonable accuracy when compared with simulated data, but in general it is difficult to determine their accuracy in any given situation. One alternative method for determining savings uncertainty to any desired degree of accuracy is to use a Bayesian approach.” Several advantages and drawbacks of Bayesian approaches are described by (Carstens, Xia, and Yadavalli (2018)). Advantages include: Because Bayesian models are probabilistic, uncertainty is automatically and exactly quantified. Confidence intervals can be interpreted in the way most people understand them: degrees of belief about the value of the parameter. Bayesian models are more universal and flexible than standard methods. Models are also modular and can be designed to suit the problem. For example, it is no different to create terms for serial correlation, or heteroscedasticity (non-constant variance) than it is to specify an ordinary linear model. The Bayesian approach allows for the incorporation of prior information where appropriate. When the savings need to be calculated for “normalised conditions”, for example, a “typical meteorological year”, rather than the conditions during the post-retrofit monitoring period, it is not possible to quantify uncertainty using current methods. However, (Shonder and Im (2012)) have shown that it can be naturally and easily quantified using the Bayesian approach. The first two points above are the most relevant to a data analyst: any arbitrary model structure can be defined to explain the data, and the exact same set of formulas can then be used to obtain the savings uncertainty after the models have been fitted. 3.1.2 Theory of Bayesian inference and prediction A Bayesian model is defined by two components: An observational model \\(p\\left(y|\\theta\\right)\\), or likelihood function, which describes the relationship between the data \\(y\\) and the model parameters \\(\\theta\\). A prior model \\(p(\\theta)\\) which encodes eventual assumptions regarding model parameters, independently of the observed data. Specifying prior densities is not mandatory. The target of Bayesian inference is the estimation of the posterior density \\(p\\left(\\theta|y\\right)\\), i.e. the probability distribution of the parameters conditioned on the observed data. As a consequence of Bayes’ rule, the posterior is proportional to the product of the two previous densities: \\[\\begin{equation} p(\\theta|y) \\propto p(y|\\theta) p(\\theta) \\end{equation}\\] This formula can be interpreted as follows: the posterior density is a compromise between assumptions and evidence brought by data. The prior can be “strong” or “weak”, to reflect for a more or less confident prior knowledge. The posterior will stray away from the prior as more data is introduced. Figure 3.1: Example of estimating a set point temperature after assuming a Normal prior distribution centred around 20°C. The dashed line is the point estimate which would have been obtained if only the data had been considered. The posterior distribution can be seen as a “refinement” of the prior, given the evidence of the data. In many applications, one is not only interested in estimating parameter values, but also the predictions \\(\\tilde{y}\\) of the observable during a new period. The distribution of \\(\\tilde{y}\\) conditioned on the observed data \\(y\\) is called the posterior predictive distribution: \\[\\begin{equation} p\\left(\\tilde{y}|y\\right) = \\int p\\left(\\tilde{y}|\\theta\\right) p\\left(\\theta|y\\right) \\mathrm{d}\\theta \\end{equation}\\] The posterior predictive distribution is an average of the model predictions over the posterior distribution of \\(\\theta\\). This formula is equivalent to the concept of using a trained model for prediction. Apart from the possibility to define prior distributions, the main specificity of Bayesian analysis is the fact that all variables are encoded as probability densities. The two main results, the parameter posterior \\(p(\\theta|y)\\) and the posterior prediction \\(p\\left(\\tilde{y}|y\\right)\\), are not only point estimates but complete distributions which include a full description of their uncertainty. 3.1.3 Bayesian formulation of common models If the practitioner wishes to use a regression model to explain the relationship between the parameters and the data, doing so in a Bayesian framework is very similar to the usual (frequentist) framework. As an example, a Bayesian model for linear regression with three parameters \\((\\theta_0,\\theta_1,\\theta_2)\\) and two explanatory variables \\((X_1,X_2)\\) may read: \\[\\begin{align} p(y|\\theta,X) &amp; = N\\left(\\theta_0, \\theta_1 X_1, \\theta_2 X_2, \\sigma\\right) \\\\ p(\\theta_i) &amp; = \\Gamma(\\alpha_i, \\beta_i) \\end{align}\\] This means that \\(y\\) follows a Normal distribution whose expectation is a linear function of \\(\\theta\\) and \\(X\\), with standard deviation \\(\\sigma\\) (the measurement error). The second equation is the prior model: in this example, each parameter is assigned a Gamma prior distribution parameterised by a shape \\(\\alpha\\) and a scale \\(\\beta\\). Other model structures can be formulated similarly: change-point models, polynomials, models with categorical variables… Bayesian modelling however allows for much more flexibility: Other distributions than the Normal distribution can be used in the observational model; Hierarchical modelling is possible: parameters can be assigned a prior distribution with parameters which have their own (hyper)prior distribution; Heteroscedasticity can be encoded by assuming a relationship between the error term and explanatory variables, etc. More complex models with latent variables have separate expressions for the respective conditional probabilities of the observations \\(y\\), latent variables \\(z\\) and parameters \\(\\theta\\). In this case, there is a joint likelihood function \\(p(y,z|\\theta)\\) and a marginal likelihood function \\(p(y|\\theta)\\) so that: \\[\\begin{equation} p(y|\\theta) = \\int p(y,z|\\theta) \\mathrm{d}z \\end{equation}\\] Other applications, such as the IPMVP option D, rely on the use of calibrated building energy simulation (BES) models. These models are described by a much larger number of parameters and equations that the simple regression models typically used for other IPMVP options. In this context, it is not feasible to fully describe BES models in the form of a simple likelihood function \\(p(y|\\theta)\\). In order to apply Bayesian uncertainty analysis to a BES model, it is possible to first approximate it with a Gaussian process (GP) model emulator. This process is denoted Bayesian calibration and was based on the seminal work of (Kennedy and O’Hagan (2001)). As opposed to the manual adjustment of building energy model parameters, Bayesian calibration explicitly quantifies uncertainties in calibration parameters, discrepancies between model predictions and observed values, as well as observation errors (Chong and Menberg (2018)). 3.1.4 Computation: Markov Chain Monte Carlo Except in a few convenient situations, the posterior distribution is not analytically tractable. In practice, rather than finding an exact solution for it, it is estimated by approximate methods. The most popular option for approximate posterior inference are Markov Chain Monte Carlo (MCMC) sampling methods. An MCMC algorithm returns a series of simulation draws \\(\\left(\\theta^{(1)},...,\\theta^{(S)}\\right)\\) which approximate the posterior distribution, provided that the sample size is large enough. \\[\\begin{equation} \\theta^{(s)} \\sim p(\\theta | y) \\end{equation}\\] where each draw \\(\\theta^{(s)}\\) contains a value for each of the parameters of the model. MCMC methods, Gaussian processes, and other methods for posterior approximation, are implemented in several software platforms. Two free and well-documented examples are: Stan, a platform for statistical modelling which interfaces with most data analysis languages (R, Python, Julia, Matlab); pyMC3, a Python library for probabilistic programming. Convergence diagnostics are an essential step after running an MCMC algorithm for a finite number of simulation draws. It is advised to have several chains run in parallel, in order to compare their distributions. The similarity of estimates between chains is assessed by the \\(\\hat{R}\\) convergence diagnostic (Vehtari et al. (2021)). The second diagnostic is the effective sample size (ESS), an estimate of the number of independent samples from the posterior distribution. Once trained, a model may be validated with the same metrics as in a standard M&amp;V approach: \\(R^2\\), NDB, CV-RMSE, F-statistic, etc. Since the posterior distribution is described by a finite (yet large) set of values, computing the statistics of any function \\(h\\) of the parameters (predictions, savings…) becomes straightforward. Because the series \\(\\left(\\theta^{(1)},...,\\theta^{(S)}\\right)\\) approximates the posterior distribution of \\(\\theta\\), then the series \\(\\left(h(\\theta^{(1)}),...,h(\\theta^{(S)})\\right)\\) approximates the posterior distribution of \\(h\\). 3.2 Workflow for one model 3.2.1 Overview As was mentioned in Sec. 1.2, inverse problems are all but trivial. It is possible that the available data is simply insufficient to bring useful inferences, but that we still try to train an unsuitable model with it. Statistical analysts need the right tools to guide model selection and training, and to warn them when there is a risk of biased inferences and predictions. This chapter is an attempt to summarize the essential points of a Bayesian workflow from a building energy perspective. Frequentist inference is also mentioned, but as a particular case of Bayesian inference. There is a very rich literature on the proper workflow for statistical inference, including the most cited reference in this book (Gelman et al. (2013)) and extensive online tutorials. Gelman divides the process of Bayesian data analysis into three steps: Setting up a full probability model; Conditioning on observed data (learning); Evaluating the fit of the model and the implications of the resulting posterior (checking and validation). Figure 3.2: A workflow for the proper specification and training of one model. Most of the workflow is similar for frequentist and Bayesian inference. This process concerns the training of a single model. An analyst however rarely attempts to analyze data with a single model. A simple model will provide biased inferences and predictions if its structure is too simple to represent the real system. In a complex model with many degrees of freedom, the unicity of the solution to the inverse problem is not guaranteed, leading to non-robust inferences and overfitting. All statistical learning lectures therefore come with guidelines for model checking, assessment and selection: this will be explained in Sec. 3.3. 3.2.2 Model specification and prior checking (to be completed) 3.2.3 Identifiability analysis (to be completed) 3.2.4 Convergence diagnostics (to be completed) 3.2.5 Residuals analysis (to be completed) 3.2.6 Posterior predictive checking (to be completed) 3.2.7 Inference diagnostics and practical identifiabiltiy (to be completed) 3.3 Model assessment and selection The “single-model training and validation” workflow shown on Fig. 3.2 is embedded in a larger process for the selection of the appropriate model complexity and structure. Two alternatives a shown on Fig. 3.3 and 3.4. The first one is the most common and intuitive: fitting models of gradually increasing complexity, keeping the ones that pass our validation checks, and comparing them in terms of predictive performance criteria. Inferences from simpler models may serve as starting values for more complex ones. This forward stepwise selection is suited to find a simple a robust model for prediction. The second alternative, on Fig. 3.4, is backward selection: starting from a high number of predictors and fitting models of decreasing complexity. Figure 3.3: A workflow of gradually increasing model complexity Figure 3.4: A workflow of gradually decreasing model complexity (to be completed) References "],["ordinary-linear-regression.html", "Chapter 4 Ordinary linear regression 4.1 Introduction to OLR 4.2 Simple linear regression with R 4.3 Bayesian linear regression with STAN", " Chapter 4 Ordinary linear regression 4.1 Introduction to OLR Linear regression models are usually the first example shown in most statistical learning lectures. They are a popular introduction to statistical modelling because of their simplicity, while their structure is flexible and applicable to quite a large range of physical systems. We consider an output variable \\(y\\), and a set of explanatory variables \\(x=(x_1,...,x_k)\\), and assume that a series of \\(n\\) values of \\(y_i\\) and \\(x_{i1},...x_{ik}\\) have been recorded. The ordinary linear regression model states that the distribution of \\(y\\) given the \\(n\\times k\\) matrix of predictors \\(X\\) is normal with a mean that is a linear function of \\(X\\): \\[\\begin{equation} E(y_i|\\theta, X) = \\beta_1 x_{i1} + ... + \\beta_k x_{ik} \\tag{4.1} \\end{equation}\\] The parameter \\(\\theta\\) is a vector of \\(k\\) coefficients which distribution is to be determined. Ordinary linear regression assumes a normal linear model in which observation errors are independent and have equal variance \\(\\sigma^2\\). Under these assumptions, along with a uniform prior distribution on \\(\\theta\\), the posterior distribution for \\(\\theta\\) conditional on \\(\\sigma\\) can be explicitely formulated: \\[\\begin{align} \\theta | \\sigma, y &amp; \\sim N\\left( \\hat{\\theta} , V_\\theta \\sigma^2\\right) \\tag{4.2} \\\\ \\hat{\\theta} &amp; = (X^T \\, X)^{-1} X^T \\, y \\tag{4.3}\\\\ V_\\theta &amp; = (X^T \\, X)^{-1} \\tag{4.4} \\end{align}\\] along with the marginal distribution of \\(\\sigma^2\\): \\[\\begin{align} \\sigma^2|y &amp; \\sim \\mathrm{Inv-}\\chi^2(n-k, s^2 ) \\tag{4.5} \\\\ s^2 &amp; = \\frac{1}{n-k}(y-X\\hat{\\theta})^T (y-X\\hat{\\theta}) \\tag{4.6} \\end{align}\\] In the words of Gelman et al. (2013) : “in the normal linear model framework, the first key statistical modelling issue is defining the variables \\(x\\) and \\(y\\), possibly using transformations, so that the conditional expectation of \\(y\\) is reasonably linear as a function of the columns of \\(X\\) with approximately normal errors.” The second main issue, related to a Bayesian analysis framework, is a proper specification of the prior distribution on the model parameters. Despite their simplicity, linear regression models can be very useful as a first insight into the heat balance of a building: they allow a quick assessment of which types of measurements have an impact on the global balance and guide the choice of more detailed models. Moreover, if a large enough amount of data is available, the estimates of some coefficients such as the HTC often turn out to be quite reliable. The ordinary linear regression model is enough to explain the variability of the data if the regression errors \\(y_i - E(y_i|\\theta, X)\\) are independent, identically distributed along a normal distribution with constant variance \\(\\sigma^2\\). If that is not the case, the model can be extended in several ways. The expected value \\(E(y_i|\\theta, X)\\) may be non-linear or include non-linear transformations of the explanatory variables. Unequal variances and correlated errors can be included by allowing a data covariance matrix \\(\\Sigma_y\\) that is not necessarily proportional to the identity matrix: \\(y \\sim N(X\\theta, \\Sigma_y)\\). A non-normal probability distribution can be used. These transformations invalidate the analytical solutions shown by Eq. (4.3) to (4.6), but we will see that Bayesian inference can treat them seamlessly. 4.2 Simple linear regression with R Example of ordinary linear regression with the standard R libraries 4.3 Bayesian linear regression with STAN Example of ordinary linear regression with STAN References "],["bayesian-mv.html", "Chapter 5 Bayesian M&amp;V 5.1 A Bayesian workflow for M&amp;V 5.2 Change-point models 5.3 IPMVP option C example", " Chapter 5 Bayesian M&amp;V 5.1 A Bayesian workflow for M&amp;V This tutorial applies the principles of Bayesian data analysis described in Sec. 3.1 to a Measurement and Verification (M&amp;V) problem. In a nutshell, M&amp;V aims at reliably assessing the energy savings that were actually gained from energy conservation measures (ECM). One of the main M&amp;V workflows, formalised by the IPMVP, is the “reporting period basis”, or “avoided consumption” method: a numerical model is trained during a baseline observation period (before ECMs are applied); the trained model is used to predict energy consumption during the reporting period (after energy conservation measures); predictions are compared with the measured consumption of the reporting period, in order to estimate adjusted energy savings This method therefore requires data to be recorded before and after implementation of the ECM, fora sufficiently long time. Fig. 5.1 shows the main steps of this method, when following a Bayesian approach. We assume that the measurement boundaries have been defined and that data and have been recorded during the baseline and reporting period respectively. Figure 5.1: Estimation of savings with uncertainty in an avoided consumption workflow. The step of model validation is not displayed As with standard approaches, choose a model structure to describe the data with, and formulate it as a likelihood function. Formulate eventual “expert knowledge” assumptions in the form of prior probability distributions. Run a MCMC (or other) algorithm to obtain a set of samples \\(\\left(\\theta^{(1)},...,\\theta^{(S)}\\right)\\) which approximates the posterior distribution of parameters conditioned on the baseline data \\(p(\\theta|y_\\mathit{base})\\). Validate the inference by checking convergence diagnostics: \\(\\hat{R}\\), ESS, etc. Validate the model by computing its predictions during the baseline period \\(p(\\tilde{y}_\\mathit{base}|y_\\mathit{base})\\). This can be done by taking all (or a representative set of) samples individually, and running a model simulation \\(\\tilde{y}_\\mathit{base}^{(s)} \\sim p(y_\\mathit{base}|\\theta=\\theta^{(s)})\\) for each. This set of simulations generates the posterior predictive distribution of the baseline period, from which any statistic can be derived (mean, median, prediction intervals for any quantile, etc.). The measures of model validation (\\(R^2\\), net determination bias, t-statistic…) can then be computed either from the mean, or from all samples in order to obtain their own probability densities. Compute the reporting period predictions in the same discrete way: each sample \\(\\theta^{(s)}\\) generates a profile \\(\\tilde{y}_\\mathit{repo}^{(s)} \\sim p(y_\\mathit{repo}|\\theta=\\theta^{(s)})\\), and this set of simulations generates the posterior predictive distribution of the reporting period. Since each reporting period prediction \\(\\tilde{y}_\\mathit{repo}^{(s)}\\) can be compared with the measured reporting period consumption \\(y_\\mathit{repo}\\), we can obtain \\(S\\) values for the energy savings, which distribution approximate the posterior probability of savings. 5.2 Change-point models Some systems are dependent on a variable, but only above or below a certain value. For example, cooling energy use may be proportional to ambient temperature, yet only above a certain threshold. When ambient temperature decreases to below the threshold, the cooling energy use does not continue to decrease, because the fan energy remains constant. In cases like these, simple regression can be improved by using a change-point linear regression. Change point models often have a better fit than a simple regression, especially when modeling energy usage for a facility. The energy signature of a building decomposes the total energy consumption (or power \\(\\Phi\\)) into three terms: heating, cooling, and other uses. Heating and cooling are then assumed to be linearly dependent on the outdoor air temperature \\(T_e\\), and only turned on conditionally on two threshold temperatures \\(T_{b1}\\) and \\(T_{b2}\\), respectively. \\[\\begin{align} E(\\Phi|\\theta, X) &amp; = \\Phi_0 + \\mathrm{HTC}_1 \\, \\left(T_{b1} - T_e\\right) &amp; \\mathrm{if} \\quad T_e \\leq T_{b1} \\\\ E(\\Phi|\\theta, X) &amp; = \\Phi_0 &amp; \\mathrm{if} \\quad T_{b1} \\leq T_e \\leq T_{b2} \\\\ E(\\Phi|\\theta, X) &amp; = \\Phi_0 + \\mathrm{HTC}_2 \\, \\left(T_e - T_{b2}\\right) &amp; \\mathrm{if} \\quad T_{b2} \\leq T_e \\end{align}\\] Data points should be averaged over long enough (at least daily) sampling times, so that the steady-state assumption formulated above can hold. \\(\\Phi_0\\) is the average baseline consumption during each sampling period, of all energy uses besides heating and cooling. Heating is turned on if the outdoor air temperature drops below a basis temperature \\(T_{b1}\\), and the heating power \\(\\Phi_h = \\mathrm{HTC}_1 \\, \\left(T_{b1} - T_e\\right)\\) is assumed proportional to a heat transfer coefficeint (HTC) value. The same reasoning is used to formulate cooling, with a “summer HTC” value that may be different from the first one. This model is therefore a piecewise linear regression model, where the switching points \\(T_{b1}\\) and \\(T_{b2}\\) are usually to be identified along with the other parameters. The appeal of the energy signature model is that the only data it requires are energy meter readings and outdoor air temperature, with a large sampling time. 5.3 IPMVP option C example library(rstan) library(tidyverse) library(lubridate) The data used in this example is the hourly energy consumption and outdoor air temperature data for 11 commercial buildings (office/retail), publicly available here: https://openei.org/datasets/dataset/consumption-outdoor-air-temperature-11-commercial-buildings We will be using two data files, respectively labeled Building 6 (Office “Pre”), and Building 6 (Office “Post”). 5.3.1 Loading and displaying the data The following block loads two separate data files: building60preoffice.csv is the baseline period file, saved into the df.base variable building60postoffice.csv is the reporting period file, saved into the df.repo variable The Date column of both files is converted into a DateTime type into a new column. Then, the baseline dataset is displayed for a first exploratory look at the data. # Baseline data: one year df.base &lt;- read_csv(&quot;data/building60preoffice.csv&quot;) %&gt;% mutate(DateTime = mdy_hm(Date), Date = as_date(DateTime)) # Post-retrofit data: one year df.repo &lt;- read_csv(&quot;data/building62postoffice.csv&quot;) %&gt;% mutate(DateTime = mdy_hm(Date), Date = as_date(DateTime)) # Plot the original data head(df.base) ## # A tibble: 6 x 4 ## Date OAT `Building 6 kW` DateTime ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 2009-01-02 41.6 23.3 2009-01-02 00:00:00 ## 2 2009-01-02 40.9 23.1 2009-01-02 01:00:00 ## 3 2009-01-02 39.5 23.7 2009-01-02 02:00:00 ## 4 2009-01-02 36.3 29.1 2009-01-02 03:00:00 ## 5 2009-01-02 32.8 35.6 2009-01-02 04:00:00 ## 6 2009-01-02 32.5 45.5 2009-01-02 05:00:00 ggplot(data = df.base) + geom_line(mapping = aes(x=DateTime, y=`Building 6 kW`)) A few interesting observations: The data has a hourly time step size. Every hour, the outdoor air temperature (OAT in °F) and energy use (kW) are available. The energy use is higher in summer and in winter than in-between. This suggests that this consumption data includes both heating and cooling appliances. Week-ends are clearly visible with a lower consumption than in the working days of the week. 5.3.2 Daily averaged data Averaging the data over daily time steps should allow to overlook the dependence between consecutive measurements. In turn, this allows using a model which will be much simpler than time series models, but will only be capable of low frequency predictions. The following block creates new datasets from the original ones: Measurements are daily averaged Temperatures are switched to °C for international suitability. A categorical variable is added to indicate week ends. Then, we plot the daily energy use \\(E\\) (kWh) versus the outdoor temperature \\(T\\) (°C) for both values of the week.end categorical variable. daily.average &lt;- function(df) { df %&gt;% group_by(Date) %&gt;% summarise(OAT = mean(OAT), E = sum(`Building 6 kW`), .groups = &#39;drop&#39; ) %&gt;% mutate(wday = wday(Date), week.end = wday==1 | wday==7, T = (OAT-32) * 5/9) } df.base.daily &lt;- daily.average(df.base) df.repo.daily &lt;- daily.average(df.repo) ggplot(data = df.base.daily) + geom_point(mapping = aes(x=T, y=E, color=week.end)) 5.3.3 Model definition After looking at the data, we can suggest using a change-point model which will include the effects of heating and cooling, and separate week ends from working days. The expected daily energy use \\(E\\) (in kWh per day) is a function of the outdoor temperature \\(T\\) and of a number of parameters: For the week-ends: \\(p(E|T) \\sim \\mathcal{N}\\left[\\alpha_1 + \\beta_{1,h}(\\tau_{1,h}-T)^+ + \\beta_{1,c}(T-\\tau_{1,c})^+, \\sigma\\right]\\) For the working days: \\(p(E|T) \\sim \\mathcal{N}\\left[\\alpha_2 + \\beta_{2,h}(\\tau_{2,h}-T)^+ + \\beta_{2,c}(T-\\tau_{2,c})^+,\\sigma\\right]\\) Where the 1 and 2 subscripts indicate week-ends and working day, respectively, and the \\(h\\) and \\(c\\) subscripts indicate heating and cooling modes. The \\(+\\) superscript indicates that a term is only applied if above zero. The two equations above mean that we expect the energy use \\(E\\) to be a normal distribution centered around a change-point model, with a constant standard deviation \\(\\sigma\\). Some particularities of Bayesian statistics are: this normal distribution can be replaced by any other probability distribution; the error term \\(\\sigma\\) can be formulated as a function of some inputs; etc. This model has 11 possible parameters, which makes it significantly more complex than an ordinary linear regression. We could simplify it by assuming that the “working days” and “week ends” mode share the same temperature thresholds for heating (\\(\\tau_{1,h}=\\tau_{2,h}\\)) or for cooling (\\(\\tau_{1,c}=\\tau_{2,c}\\)). The following method would also be exactly the same if we decided to complexify the model, for instance by assuming non-linear profiles on each side of the change points, or if we had more categorical variables. 5.3.4 Model specification with STAN In this example, we use the STAN probabilistic programming language, which allows full Bayesian statistical inference. A STAN model is a block of text which can either be written in a separate file, or in the same script as the current code. Specifying a model in STAN takes a certain learning curve, but it unlocks the full flexibility of Bayesian analysis. changepoint &lt;- &quot; functions { // This chunk is the formula for the changepoint model which will be used several times in this program real power_mean(int w, real t, vector alpha, vector beta_h, vector tau_h, vector beta_c, vector tau_c) { real a = w ? alpha[1] : alpha[2]; // condition on the type of day real heat = w ? beta_h[1] * fmax(tau_h[1]-t, 0) : beta_h[2] * fmax(tau_h[2]-t, 0) ; real cool = w ? beta_c[1] * fmax(t-tau_c[1], 0) : beta_c[2] * fmax(t-tau_c[2], 0) ; return (a + heat + cool); } } data { // This block declares all data which will be passed to the Stan model. int&lt;lower=0&gt; N_base; // number of data items in the baseline period vector[N_base] t_base; // temperature (baseline) int w_base[N_base]; // categorical variable for the week ends (baseline) vector[N_base] y_base; // outcome energy vector (baseline) int&lt;lower=0&gt; N_repo; // number of data items in the reporting period vector[N_repo] t_repo; // temperature (reporting) int w_repo[N_repo]; // categorical variable for the week ends (reporting) vector[N_repo] y_repo; // outcome energy vector (reporting) } parameters { // This block declares the parameters of the model. There are 10 parameters plus the error scale sigma vector[2] alpha; // baseline consumption (work days and week ends) vector[2] beta_h; // slopes for heating vector[2] tau_h; // threshold temperatures for heating vector[2] beta_c; // slopes for cooling vector[2] tau_c; // threshold temperatures for cooling real&lt;lower=0&gt; sigma; // error scale } model { // Assigning prior distributions on some parameters alpha ~ normal([400, 800], [150, 150]); tau_h ~ normal(8, 5); tau_c ~ normal(18, 5); beta_h ~ normal(40, 15); beta_c ~ normal(40, 15); // Observational model for (n in 1:N_base) { y_base[n] ~ normal(power_mean(w_base[n], t_base[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma); } } generated quantities { vector[N_base] y_base_pred; vector[N_repo] y_repo_pred; real savings = 0; for (n in 1:N_base) { y_base_pred[n] = normal_rng(power_mean(w_base[n], t_base[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma); } for (n in 1:N_repo) { y_repo_pred[n] = normal_rng(power_mean(w_repo[n], t_repo[n], alpha, beta_h, tau_h, beta_c, tau_c), sigma); savings += y_repo_pred[n] - y_repo[n]; } } &quot; Then, a list called model_data is created, which maps each part of the data to its appropriate variable into the STAN model. model_data &lt;- list( N_base = nrow(df.base.daily), t_base = df.base.daily$T, w_base = as.numeric(df.base.daily$week.end), y_base = df.base.daily$E, N_repo = nrow(df.repo.daily), t_repo = df.repo.daily$T, w_repo = as.numeric(df.repo.daily$week.end), y_repo = df.repo.daily$E ) 5.3.5 Model fitting Now that the model has been specified and the data has been mapped to its variables, the syntax for model fitting is below. One disadvantage of Bayesian inference is that the MCMC algorithm takes much longer to converge than a typical least-squares model fitting method. Running the code below might take a minute because we are only using 365 data points, but the Bayesian approach might become problematic for larger data files. # Fitting the model fit1 &lt;- stan( model_code = changepoint, # Stan program data = model_data, # named list of data chains = 2, # number of Markov chains warmup = 1000, # number of warmup iterations per chain iter = 4000, # total number of iterations per chain cores = 2, # number of cores (could use one per chain) refresh = 0, # progress not shown ) Fitting may result in a number of warnings, telling us that some problems may have occurred: divergent transitions, large R-hat values, low Effective Sample Size… Obtaining a fit without these warnings takes some practice but is essential for an unbiased interpretation of the inferred variables and predictions. A guide to Stan’s warnings and how to address them is available here. The first step into solving these warnings is to re-run the algorithm with different controls: iter, max_treedepth, etc. If problems persist, it is possible that the model is too complex for the information that the data is able to provide and should be simplified, or that stronger priors should be proposed. A lot of problems can be solved with some prior information. In our specific case, this is especially useful for the variables in the equation for the week-ends, since there are not a lot of data points. 5.3.6 Validation and results Stan returns an object (called fit1 above) from which the distributions of outputs and parameters of the fitted model can be accessed The MCMC algorithm produces a chain of samples \\(\\theta^{(m)}\\) for the parameters, which approximate their posterior distributions. In this case, each parameter of the model is represented by a chain of 6,000 draws: from these draws, we can extract any statistics we need: mean, median, quantiles, \\(t\\)-score and \\(p\\)-values, etc. 5.3.6.1 Parameters As a first validation step, it is useful to take a look at the values of the parameters that have been estimated by the algorithm. Below, we use three diagnostics tools: The print method shows the table of parameters, much like we could display after an ordinary linear regression traceplot shows the traces of the selected parameters. If the fitting has converged, the traces approximate the posterior distributions pairs shows the pairwise relationships between parameters. Strong interactions between some parameters are an indication that the model should be re-parameterised. print(fit1, pars = c(&quot;alpha&quot;, &quot;beta_h&quot;, &quot;tau_h&quot;, &quot;beta_c&quot;, &quot;tau_c&quot;, &quot;sigma&quot;, &quot;savings&quot;)) ## Inference for Stan model: 2f8fdf9f22a34576398be759917c5a83. ## 2 chains, each with iter=4000; warmup=1000; thin=1; ## post-warmup draws per chain=3000, total post-warmup draws=6000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## alpha[1] 448.29 1.11 38.75 349.77 430.08 452.99 473.76 508.56 1210 1 ## alpha[2] 830.09 0.20 12.69 804.25 822.00 830.57 839.03 853.25 4107 1 ## beta_h[1] 35.80 0.05 3.41 29.55 33.46 35.64 37.96 43.00 4249 1 ## beta_h[2] 33.39 0.04 2.85 27.80 31.45 33.41 35.31 39.03 4646 1 ## tau_h[1] 9.88 0.04 1.50 7.37 8.82 9.80 10.76 13.32 1450 1 ## tau_h[2] 6.50 0.01 0.81 5.19 5.93 6.42 6.99 8.33 3136 1 ## beta_c[1] 14.81 0.43 8.94 6.17 10.27 12.87 15.85 45.24 435 1 ## beta_c[2] 29.33 0.05 3.35 22.87 27.05 29.28 31.54 36.15 4225 1 ## tau_c[1] 16.11 0.17 4.43 7.71 13.60 15.84 18.01 27.25 660 1 ## tau_c[2] 15.81 0.02 1.03 13.58 15.15 15.90 16.55 17.55 3284 1 ## sigma 103.09 0.05 3.93 95.69 100.44 102.94 105.61 111.07 6743 1 ## savings 69121.90 35.23 2843.90 63555.75 67187.77 69124.93 71009.82 74685.45 6516 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Jun 23 15:06:13 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). traceplot(fit1, pars = c(&quot;alpha&quot;, &quot;beta_h&quot;, &quot;tau_h&quot;, &quot;beta_c&quot;, &quot;tau_c&quot;, &quot;sigma&quot;, &quot;lp__&quot;)) pairs(fit1, pars = c(&quot;alpha&quot;, &quot;beta_h&quot;, &quot;savings&quot;)) 5.3.6.2 Predictions Our main goal here is to compare the energy use measured during the reporting period \\(y_\\mathit{repo}\\) with the predictions of the fitted model. Since it is a probabilistic model, its outcome is actually a probability distribution \\(p\\left(y_\\mathit{repo}|x_\\mathit{repo}, x_\\mathit{base}, y_\\mathit{base}\\right)\\), based on the observed values of the model inputs \\(x\\) during the baseline and reporting periods, and on the observed energy use during the baseline period \\(y_\\mathit{base}\\). This so-called posterior predictive distribution \\(p\\left(y_\\mathit{repo}|...\\right)\\) is already directly available, because a value of \\(y_\\mathit{repo}\\) (for each time step) was directly calculated by the Stan model for each value \\(\\theta^{(m)}\\) (see Stan user’s guide for more details). \\[\\begin{equation} p\\left(y_\\mathit{repo}|...\\right) \\approx \\frac{1}{M} \\sum_{m=1}^M p\\left(y_\\mathit{repo}|x_\\mathit{repo},\\theta^{(m)}\\right) \\end{equation}\\] First, let us look at the posterior predictive distribution during the baseline period, in order to validate the model compared to its training data: # Extracting full predictive distributions from the stanfit object la &lt;- rstan::extract(fit1, permuted = TRUE) y_base_pred &lt;- la$y_base_pred # Quantiles y_base_quan &lt;- apply(y_base_pred, 2, quantile, probs=c(0.025, 0.5, 0.975)) # Data frame df.base.post &lt;- data.frame(Date = df.base.daily$Date, T = df.base.daily$T, y = df.base.daily$E, w = df.base.daily$week.end, pred_low = y_base_quan[1, ], pred_med = y_base_quan[2, ], pred_up = y_base_quan[3, ]) # Plot ggplot(data = df.base.post) + geom_point(mapping = aes(x=T, y=y, color=w)) + geom_line(data = . %&gt;% filter(!df.base.post$w), mapping = aes(x=T, y=pred_med), color=&#39;red&#39;) + geom_ribbon(data = . %&gt;% filter(!df.base.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;red&#39;, alpha=0.1) + geom_line(data = . %&gt;% filter(df.base.post$w), mapping = aes(x=T, y=pred_med), color=&#39;blue&#39;) + geom_ribbon(data = . %&gt;% filter(df.base.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;blue&#39;, alpha=0.1) The colored bands show a 95% prediction interval for the working days and the week ends, respectively. The points are the measurements of the baseline period. 5.3.7 Residuals An important validation step is to check for autocorrelation in the residuals of the fitted model, on the baseline data that was used for fitting. Autocorrelation is often a sign of insufficient model complexity, or that the form of the model error term has not been appropriately chosen. The two graphs below show: Residuals vs Date, in order to display eventual autocorrelation residuals vs Temperature ggplot(data = df.base.post) + geom_point(mapping = aes(x=Date, y=pred_med-y)) + geom_ribbon(mapping = aes(x=Date, ymin=pred_low-y, ymax=pred_up-y), alpha=0.2) ggplot(data = df.base.post) + geom_point(mapping = aes(x=T, y=pred_med-y)) + geom_ribbon(mapping = aes(x=T, ymin=pred_low-y, ymax=pred_up-y), alpha=0.2) The second graph is fine, but it seems that these is a trend in the residuals in the first few months and last few months of the year, suggesting that the model doesn’t quite capture the winter energy consumption very well. 5.3.8 Savings Our Stan model already calculates the expected output \\(y\\) of the reporting period, for each sample \\(\\theta_i\\) of the posterior distribution. We can therefore display a probability distribution for each of the data points of the reporting period, and compare it with the measured data in the same period. The following graph compares the energy use measured during the reporting period (points) with the probability distributions of energy use predicted by the model during the same period. # Extracting full predictive distributions from the stanfit object y_repo_pred &lt;- la$y_repo_pred # Quantiles y_repo_quan &lt;- apply(y_repo_pred, 2, quantile, probs=c(0.025, 0.5, 0.975)) # Data frame df.repo.post &lt;- data.frame(Date = df.repo.daily$Date, T = df.repo.daily$T, y = df.repo.daily$E, w = df.repo.daily$week.end, pred_low = y_repo_quan[1, ], pred_med = y_repo_quan[2, ], pred_up = y_repo_quan[3, ]) # Plot ggplot(data = df.repo.post) + geom_point(mapping = aes(x=T, y=y, color=w)) + geom_line(data = . %&gt;% filter(!df.repo.post$w), mapping = aes(x=T, y=pred_med), color=&#39;red&#39;) + geom_ribbon(data = . %&gt;% filter(!df.repo.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;red&#39;, alpha=0.1) + geom_line(data = . %&gt;% filter(df.repo.post$w), mapping = aes(x=T, y=pred_med), color=&#39;blue&#39;) + geom_ribbon(data = . %&gt;% filter(df.repo.post$w), mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill=&#39;blue&#39;, alpha=0.1) The savings, i.e. the difference between the measured energy use during the reporting period and their prediction by the model, have been included in the Stan model definition. Similarly to the prediction, the savings are therefore available as a probability distribution: we have a full description of any confidence interval we may wish for. The table of results shown after model fitting shows that The mean estimated savings are 69,069 kWh The 95% confidence interval spans between 63,550 and 74,880 kWh We can also choose to display any quantile of the posterior distribution of savings: plot(fit1, pars = c(&quot;savings&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) "],["finite-mixture-models.html", "Chapter 6 Finite mixture models 6.1 Principle 6.2 Example", " Chapter 6 Finite mixture models 6.1 Principle The energy signature models only offer a coarse disaggregation of energy use into three components: heating, cooling, and baseline consumption. Furthermore, they rely on very long sampling times and cannot predict sub-daily consumption profiles. Finite Mixture Models (FMM) are one way to take the disaggregation of the baseline energy consumption further. Their most common specific case are the Gaussian Mixture Models (GMM). Finite mixture models assume that the outcome \\(y\\) is drawn from one of several distributions, the identity of which is controlled by a categorical mixing distribution. For instance, the mixture of \\(K\\) normal distributions \\(f\\) with locations \\(\\mu_k\\) and scales \\(\\sigma_k\\) reads: \\[\\begin{equation} p(y_i|\\lambda, \\mu, \\sigma) = \\sum_{k=1}^K \\lambda_k f(y_i|\\mu_k,\\sigma_k) \\tag{6.1} \\end{equation}\\] where \\(\\lambda_k\\) is the (positive) mixing proportion of the \\(k\\)th component and \\(\\sum_{k=1}^K \\lambda_k = 1\\). The FMM distributes the observed values into a finite number of distributions with probability \\(\\lambda_k\\). The optimal number of components is not always a trivial choice: studies involving GMM often rely on some model selection index, such as the Bayesian Information Criterion (BIC), to guide the choice of the appropriate value for \\(K\\). The dependency of observations \\(y\\) on explanatory variables \\(x\\) can be included in the FMM, by formulating its parameters \\(\\left\\{ \\lambda_k(x), \\mu_k(x), \\sigma_k(x) \\right\\}\\) as dependent on the given value \\(x\\) of these regressors. Furthermore, in order to include the effects of different power consumption demand behaviours, the mixture probabilities \\(\\lambda_k\\) can be modelled as dependent on a categorical variable \\(z\\). Finite Mixture Models thus offer a very high flexibility for attempting to disaggregate and predict energy uses, while including the possible effects of continuous or discrete explanatory variables. 6.2 Example "],["autoregressive-models.html", "Chapter 7 Autoregressive models", " Chapter 7 Autoregressive models "],["hidden-markov-models.html", "Chapter 8 Hidden Markov models", " Chapter 8 Hidden Markov models "],["markov-switching-models.html", "Chapter 9 Markov switching models", " Chapter 9 Markov switching models "],["the-most-simple-ssm.html", "Chapter 10 The most simple SSM", " Chapter 10 The most simple SSM "],["the-kalman-filter.html", "Chapter 11 The Kalman filter", " Chapter 11 The Kalman filter "],["the-pysip-library.html", "Chapter 12 The pySIP library", " Chapter 12 The pySIP library "],["gaussian-process-models.html", "Chapter 13 Gaussian Process models", " Chapter 13 Gaussian Process models "],["references.html", "References", " References "]]
