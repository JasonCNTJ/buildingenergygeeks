<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Gaussian Process models | Building energy statistical modelling</title>
  <meta name="description" content="This is the description of the book." />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Gaussian Process models | Building energy statistical modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the description of the book." />
  <meta name="github-repo" content="srouchier/buildingenergygeeks" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Gaussian Process models | Building energy statistical modelling" />
  
  <meta name="twitter:description" content="This is the description of the book." />
  

<meta name="author" content="Simon Rouchier" />


<meta name="date" content="2021-08-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-pysip-library.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Building energy statistical modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-this-book-is-about"><i class="fa fa-check"></i>What this book is about</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-this-book-does-not-cover"><i class="fa fa-check"></i>What this book does not cover</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-about-programming-languages"><i class="fa fa-check"></i>What about programming languages?</a></li>
</ul></li>
<li class="part"><span><b>I Theory and workflow</b></span></li>
<li class="chapter" data-level="1" data-path="scope.html"><a href="scope.html"><i class="fa fa-check"></i><b>1</b> Background on data analysis</a><ul>
<li class="chapter" data-level="1.1" data-path="scope.html"><a href="scope.html#the-energy-savings-potential-of-buildings"><i class="fa fa-check"></i><b>1.1</b> The energy savings potential of buildings</a></li>
<li class="chapter" data-level="1.2" data-path="scope.html"><a href="scope.html#from-data-to-energy-savings"><i class="fa fa-check"></i><b>1.2</b> From data to energy savings</a><ul>
<li class="chapter" data-level="1.2.1" data-path="scope.html"><a href="scope.html#formalisation-of-the-system"><i class="fa fa-check"></i><b>1.2.1</b> Formalisation of the system</a></li>
<li class="chapter" data-level="1.2.2" data-path="scope.html"><a href="scope.html#some-uses-of-data"><i class="fa fa-check"></i><b>1.2.2</b> Some uses of data</a></li>
<li class="chapter" data-level="1.2.3" data-path="scope.html"><a href="scope.html#model-calibration-as-the-key-to-data-analysis"><i class="fa fa-check"></i><b>1.2.3</b> Model calibration as the key to data analysis</a></li>
<li class="chapter" data-level="1.2.4" data-path="scope.html"><a href="scope.html#inverseproblems"><i class="fa fa-check"></i><b>1.2.4</b> The difficulty of inverse problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="scope.html"><a href="scope.html#categories"><i class="fa fa-check"></i><b>1.3</b> Categories of data-driven modelling approaches</a><ul>
<li class="chapter" data-level="1.3.1" data-path="scope.html"><a href="scope.html#either-physical-interpretability-or-prediction-accuracy"><i class="fa fa-check"></i><b>1.3.1</b> Either physical interpretability or prediction accuracy</a></li>
<li class="chapter" data-level="1.3.2" data-path="scope.html"><a href="scope.html#calibrated-simulation-white-box"><i class="fa fa-check"></i><b>1.3.2</b> Calibrated simulation (white-box)</a></li>
<li class="chapter" data-level="1.3.3" data-path="scope.html"><a href="scope.html#machine-learning-black-box"><i class="fa fa-check"></i><b>1.3.3</b> Machine learning (black-box)</a></li>
<li class="chapter" data-level="1.3.4" data-path="scope.html"><a href="scope.html#statistical-modelling-and-inference-grey-box"><i class="fa fa-check"></i><b>1.3.4</b> Statistical modelling and inference (grey-box)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelling.html"><a href="modelling.html"><i class="fa fa-check"></i><b>2</b> Building energy statistical modelling</a><ul>
<li class="chapter" data-level="2.1" data-path="modelling.html"><a href="modelling.html#modelling1"><i class="fa fa-check"></i><b>2.1</b> Building physics in a nutshell</a></li>
<li class="chapter" data-level="2.2" data-path="modelling.html"><a href="modelling.html#modelling2"><i class="fa fa-check"></i><b>2.2</b> Measurement and modelling boundaries</a></li>
<li class="chapter" data-level="2.3" data-path="modelling.html"><a href="modelling.html#modelling3"><i class="fa fa-check"></i><b>2.3</b> Categories of statistical models</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>3</b> A Bayesian data analysis workflow</a><ul>
<li class="chapter" data-level="3.1" data-path="workflow.html"><a href="workflow.html#bayesian"><i class="fa fa-check"></i><b>3.1</b> Bayesian inference summarised</a><ul>
<li class="chapter" data-level="3.1.1" data-path="workflow.html"><a href="workflow.html#motivation-for-a-bayesian-approach"><i class="fa fa-check"></i><b>3.1.1</b> Motivation for a Bayesian approach</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="workflow.html"><a href="workflow.html#workflow-for-one-model"><i class="fa fa-check"></i><b>3.2</b> Workflow for one model</a><ul>
<li class="chapter" data-level="3.2.1" data-path="workflow.html"><a href="workflow.html#overview"><i class="fa fa-check"></i><b>3.2.1</b> Overview</a></li>
<li class="chapter" data-level="3.2.2" data-path="workflow.html"><a href="workflow.html#step-1-model-specification"><i class="fa fa-check"></i><b>3.2.2</b> Step 1: model specification</a></li>
<li class="chapter" data-level="3.2.3" data-path="workflow.html"><a href="workflow.html#priorpredictivechecking"><i class="fa fa-check"></i><b>3.2.3</b> Prior predictive checking</a></li>
<li class="chapter" data-level="3.2.4" data-path="workflow.html"><a href="workflow.html#computation"><i class="fa fa-check"></i><b>3.2.4</b> Step 2: computation with Markov Chain Monte Carlo</a></li>
<li class="chapter" data-level="3.2.5" data-path="workflow.html"><a href="workflow.html#modelvalidation"><i class="fa fa-check"></i><b>3.2.5</b> Step 3: model checking and validation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="workflow.html"><a href="workflow.html#modelselection"><i class="fa fa-check"></i><b>3.3</b> Model assessment and selection</a><ul>
<li class="chapter" data-level="3.3.1" data-path="workflow.html"><a href="workflow.html#model-selection-workflows"><i class="fa fa-check"></i><b>3.3.1</b> Model selection workflows</a></li>
<li class="chapter" data-level="3.3.2" data-path="workflow.html"><a href="workflow.html#sensitivity-analysis"><i class="fa fa-check"></i><b>3.3.2</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="3.3.3" data-path="workflow.html"><a href="workflow.html#structural-identifiability"><i class="fa fa-check"></i><b>3.3.3</b> Structural identifiability</a></li>
<li class="chapter" data-level="3.3.4" data-path="workflow.html"><a href="workflow.html#inferencediagnostics"><i class="fa fa-check"></i><b>3.3.4</b> Practical identifiability</a></li>
<li class="chapter" data-level="3.3.5" data-path="workflow.html"><a href="workflow.html#modelcomparison"><i class="fa fa-check"></i><b>3.3.5</b> Model comparison criteria</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Temporally independent data</b></span></li>
<li class="chapter" data-level="4" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Ordinary linear regression</a><ul>
<li class="chapter" data-level="4.1" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#introduction-to-olr"><i class="fa fa-check"></i><b>4.1</b> Introduction to OLR</a></li>
<li class="chapter" data-level="4.2" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#case-study"><i class="fa fa-check"></i><b>4.2</b> Case study</a></li>
<li class="chapter" data-level="4.3" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#simple-linear-regression-with-r"><i class="fa fa-check"></i><b>4.3</b> Simple linear regression with R</a></li>
<li class="chapter" data-level="4.4" data-path="ordinary-linear-regression.html"><a href="ordinary-linear-regression.html#bayesian-linear-regression-with-stan"><i class="fa fa-check"></i><b>4.4</b> Bayesian linear regression with STAN</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayesian-mv.html"><a href="bayesian-mv.html"><i class="fa fa-check"></i><b>5</b> Bayesian M&amp;V</a><ul>
<li class="chapter" data-level="5.1" data-path="bayesian-mv.html"><a href="bayesian-mv.html#a-bayesian-workflow-for-mv"><i class="fa fa-check"></i><b>5.1</b> A Bayesian workflow for M&amp;V</a></li>
<li class="chapter" data-level="5.2" data-path="bayesian-mv.html"><a href="bayesian-mv.html#change-point-models"><i class="fa fa-check"></i><b>5.2</b> Change-point models</a></li>
<li class="chapter" data-level="5.3" data-path="bayesian-mv.html"><a href="bayesian-mv.html#ipmvp-option-c-example"><i class="fa fa-check"></i><b>5.3</b> IPMVP option C example</a><ul>
<li class="chapter" data-level="5.3.1" data-path="bayesian-mv.html"><a href="bayesian-mv.html#loading-and-displaying-the-data"><i class="fa fa-check"></i><b>5.3.1</b> Loading and displaying the data</a></li>
<li class="chapter" data-level="5.3.2" data-path="bayesian-mv.html"><a href="bayesian-mv.html#daily-averaged-data"><i class="fa fa-check"></i><b>5.3.2</b> Daily averaged data</a></li>
<li class="chapter" data-level="5.3.3" data-path="bayesian-mv.html"><a href="bayesian-mv.html#model-definition"><i class="fa fa-check"></i><b>5.3.3</b> Model definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="bayesian-mv.html"><a href="bayesian-mv.html#model-specification-with-stan"><i class="fa fa-check"></i><b>5.3.4</b> Model specification with STAN</a></li>
<li class="chapter" data-level="5.3.5" data-path="bayesian-mv.html"><a href="bayesian-mv.html#model-fitting"><i class="fa fa-check"></i><b>5.3.5</b> Model fitting</a></li>
<li class="chapter" data-level="5.3.6" data-path="bayesian-mv.html"><a href="bayesian-mv.html#validation-and-results"><i class="fa fa-check"></i><b>5.3.6</b> Validation and results</a></li>
<li class="chapter" data-level="5.3.7" data-path="bayesian-mv.html"><a href="bayesian-mv.html#residuals"><i class="fa fa-check"></i><b>5.3.7</b> Residuals</a></li>
<li class="chapter" data-level="5.3.8" data-path="bayesian-mv.html"><a href="bayesian-mv.html#savings"><i class="fa fa-check"></i><b>5.3.8</b> Savings</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>6</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#principle"><i class="fa fa-check"></i><b>6.1</b> Principle</a></li>
<li class="chapter" data-level="6.2" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#example"><i class="fa fa-check"></i><b>6.2</b> Example</a></li>
</ul></li>
<li class="part"><span><b>III Time-series modelling</b></span></li>
<li class="chapter" data-level="7" data-path="autoregressive-models.html"><a href="autoregressive-models.html"><i class="fa fa-check"></i><b>7</b> Autoregressive models</a></li>
<li class="chapter" data-level="8" data-path="hmm.html"><a href="hmm.html"><i class="fa fa-check"></i><b>8</b> Hidden Markov models</a><ul>
<li class="chapter" data-level="8.1" data-path="hmm.html"><a href="hmm.html#principles"><i class="fa fa-check"></i><b>8.1</b> Principles</a><ul>
<li class="chapter" data-level="8.1.1" data-path="hmm.html"><a href="hmm.html#the-forward-algorithm"><i class="fa fa-check"></i><b>8.1.1</b> The forward algorithm</a></li>
<li class="chapter" data-level="8.1.2" data-path="hmm.html"><a href="hmm.html#the-viterbi-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> The Viterbi algorithm</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="hmm.html"><a href="hmm.html#example-1"><i class="fa fa-check"></i><b>8.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="markov-switching-models.html"><a href="markov-switching-models.html"><i class="fa fa-check"></i><b>9</b> Markov switching models</a><ul>
<li class="chapter" data-level="9.1" data-path="markov-switching-models.html"><a href="markov-switching-models.html#principle-1"><i class="fa fa-check"></i><b>9.1</b> Principle</a></li>
<li class="chapter" data-level="9.2" data-path="markov-switching-models.html"><a href="markov-switching-models.html#example-2"><i class="fa fa-check"></i><b>9.2</b> Example</a></li>
</ul></li>
<li class="part"><span><b>IV State-space models</b></span></li>
<li class="chapter" data-level="10" data-path="principle-of-ssms.html"><a href="principle-of-ssms.html"><i class="fa fa-check"></i><b>10</b> Principle of SSMs</a><ul>
<li class="chapter" data-level="10.1" data-path="principle-of-ssms.html"><a href="principle-of-ssms.html#description"><i class="fa fa-check"></i><b>10.1</b> Description</a></li>
<li class="chapter" data-level="10.2" data-path="principle-of-ssms.html"><a href="principle-of-ssms.html#linear-state-space-models"><i class="fa fa-check"></i><b>10.2</b> Linear state-space models</a></li>
<li class="chapter" data-level="10.3" data-path="principle-of-ssms.html"><a href="principle-of-ssms.html#the-kalman-filter"><i class="fa fa-check"></i><b>10.3</b> The Kalman filter</a></li>
<li class="chapter" data-level="10.4" data-path="principle-of-ssms.html"><a href="principle-of-ssms.html#non-linear-state-space-models"><i class="fa fa-check"></i><b>10.4</b> Non-linear state-space models</a><ul>
<li class="chapter" data-level="10.4.1" data-path="principle-of-ssms.html"><a href="principle-of-ssms.html#switching-state-space-models"><i class="fa fa-check"></i><b>10.4.1</b> Switching state-space models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="a-simple-rc-model.html"><a href="a-simple-rc-model.html"><i class="fa fa-check"></i><b>11</b> A simple RC model</a></li>
<li class="chapter" data-level="12" data-path="the-pysip-library.html"><a href="the-pysip-library.html"><i class="fa fa-check"></i><b>12</b> The pySIP library</a></li>
<li class="part"><span><b>V Gaussian Process models</b></span></li>
<li class="chapter" data-level="13" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html"><i class="fa fa-check"></i><b>13</b> Gaussian Process models</a><ul>
<li class="chapter" data-level="13.1" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#principle-2"><i class="fa fa-check"></i><b>13.1</b> Principle</a></li>
<li class="chapter" data-level="13.2" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#gaussian-processes-for-prediction-of-energy-use"><i class="fa fa-check"></i><b>13.2</b> Gaussian Processes for prediction of energy use</a></li>
<li class="chapter" data-level="13.3" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#gaussian-processes-for-time-series-data"><i class="fa fa-check"></i><b>13.3</b> Gaussian Processes for time series data</a></li>
<li class="chapter" data-level="13.4" data-path="gaussian-process-models.html"><a href="gaussian-process-models.html#latent-force-models"><i class="fa fa-check"></i><b>13.4</b> Latent Force Models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Building energy statistical modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gaussian-process-models" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Gaussian Process models</h1>
<div id="principle-2" class="section level2">
<h2><span class="header-section-number">13.1</span> Principle</h2>
<p>In machine learning, Gaussian Process (GP) regression is a widely used tool for solving modelling problems (<span class="citation">Rasmussen (<a href="#ref-rasmussen2003gaussian" role="doc-biblioref">2003</a>)</span>). The appeal of GP models comes from their flexibility and ease of encoding prior information into the model.</p>
<p>A GP is a generalization of the Gaussian probability distribution to infinite dimensions. Instead of having a mean vector and a covariance matrix, the Gaussian process <span class="math inline">\(f(\mathbf{x})\)</span> is a random function in a d-dimensional input space, characterized by a mean function <span class="math inline">\(\mu: \mathbb{R}^d \rightarrow \mathbb{R}\)</span> and a covariance function <span class="math inline">\(\kappa: \mathbb{R}^{d\times d} \rightarrow \mathbb{R}\)</span>
<span class="math display">\[\begin{equation} 
        f\left(\mathbf{x}\right) \sim \mathrm{GP}(\mu(\mathbf{x}),\,\kappa(\mathbf{x}, \mathbf{x}^\prime))
        (\#GP_model_general)
    \end{equation}\]</span></p>
<p>The variable <span class="math inline">\(\mathbf{x}\)</span> is the input of the Gaussian process and not the state vector defined in the previous section. The notation of equation @ref(GP_model_general) implies that any finite collection of random variables <span class="math inline">\(\{f(\mathbf{x}_i)\}^n_{i=1}\)</span> has a multidimensional Gaussian distribution (Gaussian process prior)
<span class="math display">\[\begin{equation}
        \left\{f(\mathbf{x}_1), f(\mathbf{x}_2), \ldots, f(\mathbf{x}_n)\right\} \sim
         \mathcal{N}(\mathbf{\mu}, \mathbf{K})
         (\#joint_GP)
    \end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{K}_{i,\,j} = \kappa(\mathbf{x}_i, \mathbf{x}_j)\)</span> defines the covariance matrix and <span class="math inline">\(\mathbf{\mu}_i = \mu(\mathbf{x}_i)\)</span> the mean vector, for <span class="math inline">\(i,j = 1,2,\ldots,n\)</span>.</p>
<p>The mean function is often, without loss of generality, fixed to zero (e.g. <span class="math inline">\(\mu(\mathbf{x}) = \mathbf{0}\)</span>) if no prior information is available; assumption regarding the mean behavior of the process can be encoded into the covariance function instead (Solin et al.). Indeed, the choice of covariance function allows encoding any prior belief about the properties of the stochatic process <span class="math inline">\(f(\mathbf{x})\)</span>, e.g. linearity, smoothness, periodicity, etc. New covariance functions can be formulated by combining existing covariance functions. The sum <span class="math inline">\(\kappa(\mathbf{x}, \mathbf{x}^\prime) = \kappa_1(\mathbf{x}, \mathbf{x}^\prime) + \kappa_2(\mathbf{x}, \mathbf{x}^\prime)\)</span>, or the product <span class="math inline">\(\kappa(\mathbf{x}, \mathbf{x}^\prime) = \kappa_1(\mathbf{x}, \mathbf{x}^\prime) \times \kappa_2(\mathbf{x}, \mathbf{x}^\prime)\)</span> of two covariance functions is a valid covariance function.</p>
<p>The Gaussian process regression is concerned by the problem of estimating the value of an unknown function <span class="math inline">\(f(t)\)</span> at arbitrary time instant <span class="math inline">\(t\)</span> (i.e. test point) based on a noisy training data <span class="math inline">\(\mathcal{D} = \left\{t_k, y_k\right\}^n_{k=1}\)</span></p>
<p><span class="math display">\[\begin{align}
    f(t) &amp; \sim \mathrm{GP}(0, \kappa(t, t^\prime)) \\
    y_k &amp; = f(t_k) + v_k
    (\#gp_model)
\end{align}\]</span></p>
<p>The joint distribution between the test point <span class="math inline">\(f(t)\)</span> and the training points <span class="math inline">\(\left(f(t_1),\,f(t_2),\,\ldots,\,f(t_n)\right)\)</span> is Gaussian with known statistics. Because the measurement model in equation @ref(gp_model) is linear and Gaussian, the joint distribution between the test point <span class="math inline">\(f(t)\)</span> and the measurements <span class="math inline">\(\left(y_1,\,y_2,\,\ldots,\,y_n\right)\)</span> is Gaussian with known statistics as well. From the property of the Gaussian distribution, the conditional distribution of <span class="math inline">\(f(t)\)</span> given the measurements has an analytical solution (<span class="citation">Särkkä and Solin (<a href="#ref-sarkka2019applied" role="doc-biblioref">2019</a>)</span>).
<span class="math display">\[\begin{equation} 
    p\left(f(t) \mid \mathbf{y}\right) =
    p\left(\mathbb{E}[f(t)] \mid \mathbb{V}[f(t)]\right)
    (@gp_posterior)
\end{equation}\]</span>
with mean and variance
<span class="math display">\[\begin{align}
    \mathbb{E}[f(t)] &amp;= \mathbf{k}^\text{T}\,\left(
        \mathbf{K} + \sigma^2_\varepsilon\,\mathbf{I}\right)^{-1}\,\mathbf{y} \\
        \mathbb{V}[f(t)] &amp;= \kappa(t, t) - \mathbf{k}^\text{T}\,
        \left(\mathbf{K} + \sigma^2_\varepsilon\,\mathbf{I}\right)^{-1}\,\mathbf{k}
         (\#gp_posterior_stats)
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{K}_{i,\,j} = \kappa(t_i, t_j)\)</span>, <span class="math inline">\(\mathbf{k} = \kappa(t, \mathbf{t})\)</span> and, <span class="math inline">\(\mathbf{t}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are the time and measurement vectors from the training data <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>The estimated function model represents dependencies between function values at different inputs through the correlation structure given by the covariance function. Thus, the function values at the observed points give information also of the unobserved points.</p>
</div>
<div id="gaussian-processes-for-prediction-of-energy-use" class="section level2">
<h2><span class="header-section-number">13.2</span> Gaussian Processes for prediction of energy use</h2>
<p>The first application of Gaussian Processes in building energy modelling is based on the developments of Kennedy and O’Hagan (<span class="citation">Kennedy and O’Hagan (<a href="#ref-kennedy2001bayesian" role="doc-biblioref">2001</a>)</span>) which they called <em>Bayesian calibration</em>. Bayesian model calibration refers to using a GP as a surrogate model to reproduce a reference model, then training a second GP as the discrepancy function between this model and observations, then evaluating the posterior distribution of calibration parameters. In this context GPs have static inputs and are not dynamic models.
<span class="math display">\[\begin{equation}
  z_i = \zeta(\mathbf{x}_i) +e_i =\rho \, \eta(\mathbf{x}_i,\theta)+\delta(\mathbf{x}_i)+e_i
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{x}_i\)</span> is a series of known model inputs, <span class="math inline">\(z_i\)</span> are observations, <span class="math inline">\(\zeta(\mathbf{x}_i)\)</span> is the true value of the real process, <span class="math inline">\(\eta(\mathbf{x}_i,\theta)\)</span> is a computer model output with parameter <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\delta(\mathbf{x}_i)\)</span> is the discrepancy function and <span class="math inline">\(e_i \sim N(0,\lambda)\)</span> are the observation errors. In Kennedy and O’Hagan’s work, GP are used to represent prior information about both <span class="math inline">\(\eta(\cdot,\cdot)\)</span> and <span class="math inline">\(\delta(\cdot)\)</span>. <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\lambda\)</span> are hyperparameters, to be added to the list of hyperparameters of the covariance functions into a global hyperparameter vector <span class="math inline">\(\phi\)</span>.</p>
<p>Before attempting prediction of the true phenomenon using the calibrated code, the first step is to derive the posterior distribution of the parameters <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\beta\)</span> (parameters of the GP mean functions) and <span class="math inline">\(\phi\)</span>. Hyperparameters are estimated in two stages: <span class="math inline">\(\eta(\cdot,\cdot)\)</span> is estimated from a series of code outputs, and <span class="math inline">\(\delta(\cdot)\)</span> is estimated from observations. The authors restrict their study to having analytical, tractable posterior distributions that do not require methods such as MCMC. Therefore they fix the value of some hyperparameters to make these functions tractable, and have to resort to some simplifications.</p>
<p>The first application of this method to building energy modelling was the work of Heo et al (<span class="citation">Heo, Choudhary, and Augenbroe (<a href="#ref-heo2012calibration" role="doc-biblioref">2012</a>)</span>). They followed the formulation of Bayesian calibration developed by Kennedy and O’Hagan, and used three sets of data as input: (1) monthly gas consumption values as observations <span class="math inline">\(y(x)\)</span>, (2) computer outputs from exploring the space of calibration parameters <span class="math inline">\(\eta(x,\theta)\)</span>, and (3) the prior PDF of calibration parameters <span class="math inline">\(p(\theta)\)</span>. The model outputs <span class="math inline">\(\eta(x,\theta)\)</span> and the bias term <span class="math inline">\(\delta(x)\)</span> are both modeled as GPs. Calibration parameters are for instance: infiltration rate, indoor temperature, <span class="math inline">\(U\)</span>-values, etc. With very little data, results are posterior PDFs which are very close to the priors.</p>
<p>GP learning scales poorly with the amount of data, which restricts its applicability to lower observation time steps. (<span class="citation">Kristensen, Choudhary, and Petersen (<a href="#ref-kristensen2017bayesian" role="doc-biblioref">2017</a>)</span>) studied the influence of time resolution on the predictive accuracy and showed the advantage of higher resolutions. More recently, (<span class="citation">Chong et al. (<a href="#ref-chong2017bayesian" role="doc-biblioref">2017</a>)</span>) used the NUTS algorithm for the MCMC sampling in order to accelerate learning. Later, (<span class="citation">Chong and Menberg (<a href="#ref-chong2018guidelines" role="doc-biblioref">2018</a>)</span>) gave a summary of publications using Bayesian calibration in building energy. In (<span class="citation">Gray and Schmidt (<a href="#ref-gray2018hybrid" role="doc-biblioref">2018</a>)</span>), a hybrid model was implemented. A zero mean GP is trained to learn the error between the grey-box model and the reference data. As in the previous references, both models are added to obtain the final predicted output. They are trained in sequence: the GB model has some inputs <span class="math inline">\(\mathbf{u}_\mathrm{GB}\)</span> and is trained first; then the GP has some other inputs <span class="math inline">\(\mathbf{u}_\mathrm{GP}\)</span> and is trained on the GB model’s prediction error. Results are the hyperparameters of the GP.</p>
<p>Models trained by this method are said to have very good prediction performance, since the GP predicts the inadequacy of the GB as a function of new inputs, not included in the physical model. However, the method may not be fit for the interpretation of physical parameters. Indeed, since the GB model is first trained independently from the GP, it is biased and its parameter estimates are not interpretable.</p>
</div>
<div id="gaussian-processes-for-time-series-data" class="section level2">
<h2><span class="header-section-number">13.3</span> Gaussian Processes for time series data</h2>
<p>Gaussian process are non-parametric models, which means that the latent function <span class="math inline">\(f(t)\)</span> is represented by an infinite-dimensional parameter space. Unlike parametric methods, the number of parameters is not fixed, but grows with the size of the dataset <span class="math inline">\(\mathcal{D}\)</span>, which is an advantage and a limitation. Non-parametric models are memory-based, which means that they can represent more complex mapping as the data set grows but in order to make predictions they have to “remember” the full dataset (<span class="citation">Frigola (<a href="#ref-frigola2015bayesian" role="doc-biblioref">2015</a>)</span>).</p>
<p>The computational complexities of the analytical regression equations @ref(gp_posterior_stats) are cubic <span class="math inline">\(\mathcal{O}(N^3)\)</span> in the number of measurements <span class="math inline">\(N\)</span>, which is not suited for long time series. However, for a certain class of covariance function, temporal Gaussian process regression is equivalent to state inference problem which can be solved with Kalman filter and Rauch-Tung-Striebel smoother (<span class="citation">Hartikainen and Särkkä (<a href="#ref-hartikainen2010kalman" role="doc-biblioref">2010</a>)</span>). The computational complexity of these sequential methods is linear <span class="math inline">\(\mathcal{O}(N)\)</span> instead of cubic in the number of measurements <span class="math inline">\(N\)</span>.</p>
<p>A stationary Gaussian process (i.e. the covariance function depends only on the time difference <span class="math inline">\(\kappa(t, t^\prime) = \kappa(\tau)\)</span>, with <span class="math inline">\(\tau=\lvert t - t^\prime \rvert\)</span>) can be exactly represented or well approximated by a stochastic state-space model:
<span class="math display">\[\begin{align}
        \mathrm{d}\mathbf{f} &amp; = \mathbf{A_{gp}} \, \mathbf{f} \, \mathrm{d}t
            + \mathbf{\sigma}_{\mathbf{gp}} \, \mathrm{d}\mathbf{w} \\
        y_k &amp; = \mathbf{C}_{\mathbf{gp}} \, \mathbf{f}(t_k) + v_k
(\#gp_ssm)
\end{align}\]</span>
where the matrices of the system are defined by the choice of covariance function.</p>
<p>A list of widely used covariance function with this dual representation is given in (<span class="citation">Solin and others (<a href="#ref-solin2016stochastic" role="doc-biblioref">2016</a>)</span>), (<span class="citation">Särkkä and Solin (<a href="#ref-sarkka2019applied" role="doc-biblioref">2019</a>)</span>). As example, consider the Mat'ern covariance function with decay parameter <span class="math inline">\(\nu=3/2\)</span>
<span class="math display">\[\begin{equation}
    \kappa \left(\tau\right) = \sigma^2 \, \left(1 + \frac{\sqrt{3}\tau}{\ell}\right) \,
    \exp\left(-\frac{\sqrt{3}\tau}{\ell}\right)
     (\#matern_covariance)
\end{equation}\]</span>
which has the following equivalent state-space representation
<span class="math display">\[\begin{equation}
    \mathbf{A_{gp}} = \begin{pmatrix}
        0 &amp; 1 \\[0.5em]
        -\lambda^2 &amp; -2\lambda
    \end{pmatrix}
    \quad
    \mathbf{\sigma}_{\mathbf{gp}} = \begin{pmatrix}
        0 &amp; 0 \\[0.5em]
        0 &amp; 2\lambda^{3/2}\sigma
    \end{pmatrix}
    \quad
    \mathbf{C}_{\mathbf{gp}} =
        \begin{pmatrix} 1 &amp; 0 \end{pmatrix}
 (\#matern_ssm)
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\lambda=\sqrt{2\,\nu} / \ell\)</span> and where <span class="math inline">\(\sigma, \ell &gt; 0\)</span> are the magnitude and length-scale parameters.</p>
<p>The parameter <span class="math inline">\(\ell\)</span> controls the smoothness (i.e. how much time difference <span class="math inline">\(\tau\)</span> is required to observe a significant change in the function value) and the parameter <span class="math inline">\(\sigma\)</span> controls the overall variance of the function (i.e. the expected magnitude of function values).</p>
</div>
<div id="latent-force-models" class="section level2">
<h2><span class="header-section-number">13.4</span> Latent Force Models</h2>
<p>The stochastic part of the state-space model can accommodate for unmodelled disturbances, which do not have a significant influence on the thermal dynamics. This assumption holds if the disturbances have white noise properties and are uncorrelated accross time lags, which is seldom the case in practice (<span class="citation">Ghosh et al. (<a href="#ref-ghosh2015modeling" role="doc-biblioref">2015</a>)</span>). Usually, the model complexity is increased to erase the structure in the model residuals. However, this strategy may lead to unnecessarily complex models because non-linear dynamics are often modelled by linear approximations. Increasing the model complexity often requires more prior knowledge about the underlying physical systems and additional measurements, which may not be available in practice.</p>
<p>Another strategy is to model these unknown disturbances as Gaussian processes with certain parametrized covariance structures (<span class="citation">Särkkä, Álvarez, and Lawrence (<a href="#ref-sarkka2018gaussian" role="doc-biblioref">2018</a>)</span>). The resulting latent force model (<span class="citation">Alvarez, Luengo, and Lawrence (<a href="#ref-alvarez2009latent" role="doc-biblioref">2009</a>)</span>) is a combination of parametric grey-box model and non-parametric Gaussian process model.
<span class="math display">\[\begin{align}
        \mathrm{d}\mathbf{x} &amp; = \left(\mathbf{A_{rc} \, \mathbf{x}
            + \mathbf{M_{rc}} \, \mathbf{C_{gp}}\mathbf{f}
            + \mathbf{B_{rc}} \, \mathbf{u}} \right) \, \mathrm{d}t
            + \mathbf{\sigma}_{\mathbf{rc}} \, \mathrm{d}\mathbf{w} \\
        \mathrm{d}\mathbf{f} &amp;= \mathbf{A_{gp}} \, \mathbf{f} \, \mathrm{d}t
            + \mathbf{\sigma}_{\mathbf{gp}} \, \mathrm{d}\mathbf{w} \\
        y_k &amp; = \mathbf{C}_{\mathbf{rc}} \, \mathbf{x}(t_k) + v_k
    (\#lfm_sde)
\end{align}\]</span>
where <span class="math inline">\(\mathbf{M_{rc}}\)</span> is the input matrix corresponding to the unknown latent forces.</p>
<p>The augmented state-space representation of the latent force model
<span class="math display">\[\begin{align} \label{lfm_ssm}
        \mathrm{d}\mathbf{z} &amp;= \mathbf{A} \, \mathbf{z} \, \mathrm{d}t
            + \mathbf{B} \, \mathbf{u} \, \mathrm{d}t
            + \mathbf{\sigma} \, \mathrm{d}\mathbf{w} \\
        y_k &amp;= \mathbf{C} \, \mathbf{z}(t_k) + v_k
\end{align}\]</span>
is obtained by combining the grey-box model and the gaussian process model @ref(gp_ssm), such that
<span class="math display">\[\begin{equation}
\begin{alignedat}{3}
\mathbf{z}&amp;=\begin{pmatrix}
    \mathbf{x} \\
    \mathbf{f}
\end{pmatrix}
\quad &amp;
\mathbf{A}&amp;=\begin{pmatrix}
    \mathbf{A_{rc}} &amp;  \mathbf{M_{rc}} \, \mathbf{C_{gp}} \\
    \mathbf{0} &amp; \mathbf{A_{gp}}
\end{pmatrix}
\quad &amp;
\mathbf{B}=\begin{pmatrix}
    \mathbf{B_{rc}} \\
    \mathbf{0}
\end{pmatrix}
\\
\mathbf{C}&amp;=\begin{pmatrix} \mathbf{C}_{\mathbf{rc}} &amp; \mathbf{0} \end{pmatrix}
\quad &amp;
\mathbf{\sigma}&amp;=\begin{pmatrix}
    \mathbf{\sigma}_{\mathbf{rc}} &amp;  \mathbf{0} \\
    \mathbf{0} &amp; \mathbf{\sigma}_{\mathbf{gp}}
\end{pmatrix}
\end{alignedat}
\end{equation}\]</span></p>
<p>The latent force model representation allows to incorporate prior information about the overall dynamic of the physical system, but also about the behavior of the unknown inputs.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-alvarez2009latent">
<p>Alvarez, Mauricio, David Luengo, and Neil D Lawrence. 2009. “Latent Force Models.” In <em>Artificial Intelligence and Statistics</em>, 9–16. PMLR.</p>
</div>
<div id="ref-chong2017bayesian">
<p>Chong, Adrian, Khee Poh Lam, Matteo Pozzi, and Junjing Yang. 2017. “Bayesian Calibration of Building Energy Models with Large Datasets.” <em>Energy and Buildings</em> 154: 343–55.</p>
</div>
<div id="ref-chong2018guidelines">
<p>Chong, Adrian, and Kathrin Menberg. 2018. “Guidelines for the Bayesian Calibration of Building Energy Models.” <em>Energy and Buildings</em> 174: 527–47.</p>
</div>
<div id="ref-frigola2015bayesian">
<p>Frigola, Roger. 2015. “Bayesian Time Series Learning with Gaussian Processes.” PhD thesis, University of Cambridge.</p>
</div>
<div id="ref-ghosh2015modeling">
<p>Ghosh, Siddhartha, Steve Reece, Alex Rogers, Stephen Roberts, Areej Malibari, and Nicholas R Jennings. 2015. “Modeling the Thermal Dynamics of Buildings: A Latent-Force-Model-Based Approach.” <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 6 (1): 1–27.</p>
</div>
<div id="ref-gray2018hybrid">
<p>Gray, Francesco Massa, and Michael Schmidt. 2018. “A Hybrid Approach to Thermal Building Modelling Using a Combination of Gaussian Processes and Grey-Box Models.” <em>Energy and Buildings</em> 165: 56–63.</p>
</div>
<div id="ref-hartikainen2010kalman">
<p>Hartikainen, Jouni, and Simo Särkkä. 2010. “Kalman Filtering and Smoothing Solutions to Temporal Gaussian Process Regression Models.” In <em>2010 Ieee International Workshop on Machine Learning for Signal Processing</em>, 379–84. IEEE.</p>
</div>
<div id="ref-heo2012calibration">
<p>Heo, Yeonsook, Ruchi Choudhary, and GA Augenbroe. 2012. “Calibration of Building Energy Models for Retrofit Analysis Under Uncertainty.” <em>Energy and Buildings</em> 47: 550–60.</p>
</div>
<div id="ref-kennedy2001bayesian">
<p>Kennedy, Marc C, and Anthony O’Hagan. 2001. “Bayesian Calibration of Computer Models.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 63 (3): 425–64.</p>
</div>
<div id="ref-kristensen2017bayesian">
<p>Kristensen, Martin Heine, Ruchi Choudhary, and Steffen Petersen. 2017. “Bayesian Calibration of Building Energy Models: Comparison of Predictive Accuracy Using Metered Utility Data of Different Temporal Resolution.” <em>Energy Procedia</em> 122: 277–82.</p>
</div>
<div id="ref-rasmussen2003gaussian">
<p>Rasmussen, Carl Edward. 2003. “Gaussian Processes in Machine Learning.” In, 63–71. Springer.</p>
</div>
<div id="ref-sarkka2018gaussian">
<p>Särkkä, Simo, Mauricio A Álvarez, and Neil D Lawrence. 2018. “Gaussian Process Latent Force Models for Learning and Stochastic Control of Physical Systems.” <em>IEEE Transactions on Automatic Control</em> 64 (7): 2953–60.</p>
</div>
<div id="ref-sarkka2019applied">
<p>Särkkä, Simo, and Arno Solin. 2019. <em>Applied Stochastic Differential Equations</em>. Vol. 10. Cambridge University Press.</p>
</div>
<div id="ref-solin2016stochastic">
<p>Solin, Arno, and others. 2016. “Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression.”</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-pysip-library.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/E01-gaussianprocess.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["buildingenergygeeks.pdf", "buildingenergygeeks.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
